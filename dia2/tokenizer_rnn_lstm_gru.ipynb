{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizadores y capas de embeddings\n",
        "Este notebook tiene los siguientes temarios que se ha aprendido en la teoria:\n",
        "1. **Motivaci√≥n**: texto discreto ‚Üí n√∫meros\n",
        "2. **Tokenizaci√≥n**: $x \\to (t_1,\\dots,t_T)$\n",
        "3. **Embeddings**: $t_i \\to e_i \\in \\mathbb{R}^d$\n",
        "4. **Subword tokenization**: BPE, WordPiece, Unigram LM, Byte-level BPE\n",
        "5. **Tokens especiales** y **padding**\n",
        "6. **Embeddings posicionales** (intuici√≥n)\n",
        "\n",
        "Incluye **ejercicios intercalados**."
      ],
      "metadata": {
        "id": "ZOyTFuID5enu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_9gCBj0l5vNm",
        "outputId": "4ad81895-efe9-4f73-babb-0161b20c6e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Reproducibilidad (semillas)\n",
        "En ML es normal que los resultados cambien por inicializaciones aleatorias.\n",
        "Fijar semilla hace los experimentos repetibles."
      ],
      "metadata": {
        "id": "i6NeN__y5lhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "print('Seed set with value', seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESwgpavt5z3p",
        "outputId": "36b283bf-0bad-4d0b-fd6c-11eae71dc9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set with value 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Motivaci√≥n: texto discreto vs redes neuronales\n",
        "Un texto es una secuencia de s√≠mbolos discretos (caracteres/palabras). Las redes trabajan con **tensores num√©ricos**.\n",
        "\n",
        "Formalmente:\n",
        "$$\n",
        "x \\xrightarrow{\\mathcal{T}} (t_1,\\dots,t_T) \\xrightarrow{\\mathcal{E}} (e_1,\\dots,e_T),\\quad e_i\\in\\mathbb{R}^d.\n",
        "$$\n",
        "\n",
        "- $\\mathcal{T}$: tokenizador (determinista) que mapea texto ‚Üí tokens/IDs\n",
        "- $\\mathcal{E}$: embeddings (matriz entrenable) que mapea IDs ‚Üí vectores"
      ],
      "metadata": {
        "id": "EYSylaI_5_yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Tokenizador m√°s simple: whitespace + vocab\n",
        "Vamos a construir un tokenizador **de palabras** (whitespace).\n",
        "\n",
        "### Conceptos clave\n",
        "- **Vocabulario** $\\mathcal{V}$: conjunto finito de tokens\n",
        "- **IDs**: cada token tiene un entero\n",
        "- **OOV**: palabras fuera del vocabulario ‚Üí `<unk>` (si existe)"
      ],
      "metadata": {
        "id": "J8nxzetw6DQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "@dataclass\n",
        "class Vocab:\n",
        "    stoi: Dict[str, int]\n",
        "    itos: List[str]\n",
        "    pad: str = '<pad>'\n",
        "    unk: str = '<unk>'\n",
        "    bos: str = '<bos>'\n",
        "    eos: str = '<eos>'\n",
        "\n",
        "    @property\n",
        "    def pad_id(self): return self.stoi[self.pad]\n",
        "    @property\n",
        "    def unk_id(self): return self.stoi[self.unk]\n",
        "    @property\n",
        "    def bos_id(self): return self.stoi[self.bos]\n",
        "    @property\n",
        "    def eos_id(self): return self.stoi[self.eos]\n",
        "\n",
        "def build_vocab(texts: List[str], min_freq: int = 1) -> Vocab:\n",
        "    counter = Counter()\n",
        "    for t in texts:\n",
        "        counter.update(t.strip().split())\n",
        "    specials = ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "    itos = specials + [w for w,c in counter.items() if c >= min_freq and w not in specials]\n",
        "    stoi = {w:i for i,w in enumerate(itos)}\n",
        "    return Vocab(stoi=stoi, itos=itos)\n",
        "\n",
        "def encode(vocab: Vocab, text: str, add_bos_eos: bool=False) -> List[int]:\n",
        "    toks = text.strip().split()\n",
        "    ids = [vocab.stoi.get(w, vocab.unk_id) for w in toks]\n",
        "    if add_bos_eos:\n",
        "        ids = [vocab.bos_id] + ids + [vocab.eos_id]\n",
        "    return ids\n",
        "\n",
        "def decode(vocab: Vocab, ids: List[int]) -> str:\n",
        "    return ' '.join(vocab.itos[i] if 0 <= i < len(vocab.itos) else '<oov>' for i in ids)\n",
        "\n",
        "def pad_batch(seqs: List[List[int]], pad_id: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "    T = int(lengths.max().item())\n",
        "    out = torch.full((len(seqs), T), pad_id, dtype=torch.long)\n",
        "    for i,s in enumerate(seqs):\n",
        "        out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "    return out, lengths\n",
        "\n",
        "print('Tokenizer utils ready')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqQWKCP35-CK",
        "outputId": "c89c453d-4912-4ae7-dcc7-0b5785fc1613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer utils ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset de juguete\n",
        "Usaremos un mini-corpus para construir el vocabulario."
      ],
      "metadata": {
        "id": "YUxSwANc6Kmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'Me gusta la pizza',\n",
        "    'Me gusta aprender NLP',\n",
        "    'La pizza gusta a mucha gente',\n",
        "    'Aprender embeddings ayuda en NLP'\n",
        "]\n",
        "\n",
        "vocab = build_vocab(corpus, min_freq=1)\n",
        "print('Vocab size:', len(vocab.itos))\n",
        "print('Primeros tokens:', vocab.itos[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLZUWVir6IXU",
        "outputId": "89206d16-e249-4f44-f943-47f427dea216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 18\n",
            "Primeros tokens: ['<pad>', '<unk>', '<bos>', '<eos>', 'Me', 'gusta', 'la', 'pizza', 'aprender', 'NLP', 'La', 'a', 'mucha', 'gente', 'Aprender']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1 (tokenizaci√≥n b√°sica)\n",
        "**Objetivo**: comprobar que entiendes vocab, `<unk>`, y `<bos>/<eos>`.\n",
        "\n",
        "**TODO**:\n",
        "1) Tokeniza la frase: `\"Me gusta la electroencefalografista\"`\n",
        "2) Observa qu√© pasa con la palabra rara (OOV)\n",
        "3) Repite con `add_bos_eos=True`\n"
      ],
      "metadata": {
        "id": "q1Uzwscr6RDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Por qu√© el padding es necesario\n",
        "En un batch, las secuencias tienen longitudes distintas. Para formar un tensor rectangular `(B, T)` se usa `<pad>`.\n",
        "\n",
        "**Error t√≠pico**:\n",
        "```python\n",
        "torch.tensor([[1,2,3],[1,2]])  # falla\n",
        "```\n",
        "porque las filas no tienen la misma longitud."
      ],
      "metadata": {
        "id": "zw3WqF8d6j22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = [encode(vocab, s, add_bos_eos=True) for s in corpus]\n",
        "print('Longitudes:', [len(s) for s in seqs])\n",
        "\n",
        "x_pad, lengths = pad_batch(seqs, vocab.pad_id)\n",
        "print('x_pad shape:', x_pad.shape)\n",
        "print('x_pad:\\n', x_pad)\n",
        "print('lengths:', lengths.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6DIRbyc6iZM",
        "outputId": "6022b83d-db70-40c2-b15a-0a9504880f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitudes: [6, 6, 8, 7]\n",
            "x_pad shape: torch.Size([4, 8])\n",
            "x_pad:\n",
            " tensor([[ 2,  4,  5,  6,  7,  3,  0,  0],\n",
            "        [ 2,  4,  5,  8,  9,  3,  0,  0],\n",
            "        [ 2, 10,  7,  5, 11, 12, 13,  3],\n",
            "        [ 2, 14, 15, 16, 17,  9,  3,  0]])\n",
            "lengths: [6, 6, 8, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 2 (padding)\n",
        "**TODO**:\n",
        "1) Elige un ejemplo i\n",
        "2) Recupera la secuencia original eliminando `<pad>` usando `lengths`\n",
        "3) Decodif√≠cala a texto"
      ],
      "metadata": {
        "id": "-9dN1jd569oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Embeddings: matriz entrenable + lookup\n",
        "Una capa de embeddings es una matriz:\n",
        "$$E \\in \\mathbb{R}^{|\\mathcal{V}|\\times d}$$\n",
        "y el embedding de un token `i` es la fila `E[i]`.\n",
        "\n",
        "En PyTorch: `nn.Embedding(V, d)`."
      ],
      "metadata": {
        "id": "BMKMIU9T7VSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 16\n",
        "emb = nn.Embedding(num_embeddings=len(vocab.itos), embedding_dim=d_model, padding_idx=vocab.pad_id).to(device)\n",
        "\n",
        "x = x_pad.to(device)  # (B,T)\n",
        "y = emb(x)            # (B,T,d)\n",
        "print('x:', x.shape, '-> y:', y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YiwH7lk7AmH",
        "outputId": "84e8ebbd-2c72-4a9e-e10e-cf1dd05c11fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: torch.Size([4, 8]) -> y: torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lookup expl√≠cito (para entender qu√© hace Embedding)\n",
        "Esto es equivalente conceptualmente a indexar una matriz."
      ],
      "metadata": {
        "id": "50ncvB_77mRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E = emb.weight  # (V,d)\n",
        "token_id = x[0,0].item()\n",
        "print('token:', vocab.itos[token_id], '| id:', token_id)\n",
        "print('E[token_id] shape:', E[token_id].shape)\n",
        "print('embedding igual a emb(x)[0,0]? ', torch.allclose(E[token_id], y[0,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyhrOEaE7irn",
        "outputId": "190f70c3-1944-4db0-b346-be9d8faaa05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token: <bos> | id: 2\n",
            "E[token_id] shape: torch.Size([16])\n",
            "embedding igual a emb(x)[0,0]?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Interpretaci√≥n geom√©trica: similitud coseno\n",
        "En embeddings, tokens con contextos similares tienden a acabar cerca.\n",
        "Aqu√≠ solo veremos la mec√°nica (estos embeddings a√∫n son aleatorios).\n"
      ],
      "metadata": {
        "id": "8Hyi3-Hn7-CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(u, v, eps=1e-8):\n",
        "    return float((u @ v) / (u.norm()*v.norm() + eps))\n",
        "\n",
        "w1, w2 = 'pizza', 'NLP'\n",
        "id1 = vocab.stoi.get(w1, vocab.unk_id)\n",
        "id2 = vocab.stoi.get(w2, vocab.unk_id)\n",
        "print(w1, w2, 'cos:', cosine(emb.weight[id1].detach().cpu(), emb.weight[id2].detach().cpu()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdJmfdn88AWk",
        "outputId": "e73dee6b-199a-4009-d6d6-c7420a412014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pizza NLP cos: -0.038911353796720505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Tokenizaci√≥n por subpalabras (BPE, WordPiece, Unigram, Byte-level BPE)\n",
        "Hasta ahora usamos tokens = palabras (whitespace). Eso tiene problemas:\n",
        "- vocab crece mucho\n",
        "- OOV (palabras raras) aparecen\n",
        "\n",
        "Los **subwords** resuelven esto segmentando palabras raras en piezas frecuentes.\n",
        "\n",
        "En Colab instalamos librer√≠as:\n",
        "- `tokenizers` (HuggingFace): BPE, WordPiece, Byte-level BPE\n",
        "- `sentencepiece`: Unigram LM"
      ],
      "metadata": {
        "id": "ipxOlIEj8CLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tokenizers sentencepiece"
      ],
      "metadata": {
        "id": "elvHYhqO8D3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_corpus = [\n",
        "    'Me gusta aprender NLP con PyTorch.',\n",
        "    'Los tokenizadores BPE y WordPiece segmentan palabras.',\n",
        "    'Unigram LM aprende probabilidades de subpalabras.',\n",
        "    'Byte-level BPE trabaja a nivel de bytes (√∫til para cualquier texto).',\n",
        "    'Transformers usan subword tokenization para manejar OOV.',\n",
        "    'Hoy entrenamos BPE, WordPiece, Unigram y Byte-level BPE.'\n",
        "]\n",
        "\n",
        "with open('sub_corpus.txt','w',encoding='utf-8') as f:\n",
        "    for line in sub_corpus:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "print('Lines:', len(sub_corpus))\n",
        "print(sub_corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5xb0qMS8J8j",
        "outputId": "316c7c85-fa15-4e78-d562-0bd2167ff7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines: 6\n",
            "Me gusta aprender NLP con PyTorch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1) BPE (Byte Pair Encoding)\n",
        "Idea: empezar con s√≠mbolos (caracteres) y **fusionar pares frecuentes** iterativamente.\n",
        "Tokenizaci√≥n final: aplica las fusiones (greedy)."
      ],
      "metadata": {
        "id": "03O6eftPJ-HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tok_bpe = Tokenizer(BPE(unk_token='[UNK]'))\n",
        "tok_bpe.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_bpe = BpeTrainer(vocab_size=250, min_frequency=1,\n",
        "                         special_tokens=['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]'])\n",
        "tok_bpe.train(['sub_corpus.txt'], trainer_bpe)\n",
        "\n",
        "text = 'electroencefalografista aprende NLP'\n",
        "enc = tok_bpe.encode(text)\n",
        "print('TEXT:', text)\n",
        "print('TOKENS:', enc.tokens)\n",
        "print('DECODE:', tok_bpe.decode(enc.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8asd-dOJ8zV",
        "outputId": "3d028154-9c83-4f2b-9954-771e7d3c26d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT: electroencefalografista aprende NLP\n",
            "TOKENS: ['el', 'e', 'c', 't', 'r', 'o', 'en', 'ce', 'f', 'al', 'o', 'gra', 'f', 'i', 's', 'ta', 'aprende', 'NLP']\n",
            "DECODE: el e c t r o en ce f al o gra f i s ta aprende NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 4 (BPE)\n",
        "**TODO**:\n",
        "1) Tokeniza un texto con emojis y acentos\n",
        "2) Observa si aparece `[UNK]`\n",
        "3) Cambia `vocab_size` y mira c√≥mo cambian los tokens\n"
      ],
      "metadata": {
        "id": "73oTzBNEKSEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2) WordPiece\n",
        "WordPiece se usa en BERT. En pr√°ctica, tambi√©n aprende subwords con un criterio tipo verosimilitud.\n",
        "Suele marcar subwords internos con un prefijo (p.ej. `##`)."
      ],
      "metadata": {
        "id": "Ki1_YhUHKgYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "tok_wp = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
        "tok_wp.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_wp = WordPieceTrainer(vocab_size=250, min_frequency=1,\n",
        "                              special_tokens=['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]'])\n",
        "tok_wp.train(['sub_corpus.txt'], trainer_wp)\n",
        "\n",
        "text = 'jugando aprendemos tokenizacion'\n",
        "enc = tok_wp.encode(text)\n",
        "print('TEXT:', text)\n",
        "print('TOKENS:', enc.tokens)\n",
        "print('DECODE:', tok_wp.decode(enc.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrzNEgugKZL-",
        "outputId": "d1d323d3-c6e1-47d9-c86f-e51e42a68078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT: jugando aprendemos tokenizacion\n",
            "TOKENS: ['j', '##u', '##g', '##an', '##d', '##o', 'aprende', '##mos', 'tokeniz', '##a', '##c', '##i', '##on']\n",
            "DECODE: j ##u ##g ##an ##d ##o aprende ##mos tokeniz ##a ##c ##i ##on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3) Unigram LM (SentencePiece)\n",
        "Unigram parte de un vocab grande y **prunea** tokens, manteniendo los que maximizan la probabilidad.\n",
        "Puede haber varias segmentaciones; elige la m√°s probable.\n",
        "\n",
        "Usaremos `sentencepiece`."
      ],
      "metadata": {
        "id": "W8-j0VQ7Kmdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='sub_corpus.txt',\n",
        "    model_prefix='sp_unigram',\n",
        "    vocab_size=83,\n",
        "    model_type='unigram',\n",
        "    character_coverage=1.0,\n",
        "    bos_id=1, eos_id=2, pad_id=0, unk_id=3\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('sp_unigram.model')\n",
        "\n",
        "text = 'desconocido electroencefalografista'\n",
        "pieces = sp.encode(text, out_type=str)\n",
        "ids = sp.encode(text, out_type=int)\n",
        "print('TEXT:', text)\n",
        "print('PIECES:', pieces)\n",
        "print('DECODE:', sp.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_N-yFTzKkXi",
        "outputId": "380d1611-30fe-48d7-a326-c334e648029b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT: desconocido electroencefalografista\n",
            "PIECES: ['‚ñÅ', 'de', 's', 'c', 'o', 'n', 'o', 'c', 'i', 'd', 'o', '‚ñÅ', 'e', 'l', 'e', 'c', 't', 'r', 'o', 'e', 'n', 'c', 'e', 'f', 'al', 'o', 'g', 'ra', 'f', 'i', 's', 'ta']\n",
            "DECODE: desconocido electroencefalografista\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4) Byte-level BPE\n",
        "Variante BPE donde los s√≠mbolos iniciales son **bytes (0..255)**.\n",
        "Ventaja: cobertura total Unicode (casi nunca necesitas `<unk>`).\n",
        "\n",
        "Lo entrenamos con `ByteLevelBPETokenizer` (HuggingFace)."
      ],
      "metadata": {
        "id": "WAQsQjzyKu0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tok_bbpe = ByteLevelBPETokenizer()\n",
        "tok_bbpe.train(['sub_corpus.txt'], vocab_size=250, min_frequency=1,\n",
        "               special_tokens=['<s>','<pad>','</s>','<unk>','<mask>'])\n",
        "\n",
        "text = 'Byte-level BPE: Ê±âÂ≠ó, emojis üôÇ, acentos √°√©√≠√≥√∫'\n",
        "enc = tok_bbpe.encode(text)\n",
        "print('TEXT:', text)\n",
        "print('TOKENS (primeros 60):', enc.tokens[:60])\n",
        "print('¬ø<unk> aparece?', '<unk>' in enc.tokens)\n",
        "print('DECODE:', tok_bbpe.decode(enc.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoYw-T0RKrZc",
        "outputId": "1a77b072-753f-457d-8ac8-681a25aaf4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT: Byte-level BPE: Ê±âÂ≠ó, emojis üôÇ, acentos √°√©√≠√≥√∫\n",
            "TOKENS (primeros 60): ['B', 'y', 't', 'e', '-', 'l', 'e', 'v', 'e', 'l', 'ƒ†', 'B', 'P', 'E', ':', 'ƒ†', '√¶', '¬±', 'ƒ´', '√•', '≈É', 'ƒπ', ',', 'ƒ†', 'e', 'm', 'o', 'j', 'i', 's', 'ƒ†', '√∞', '≈Å', 'ƒª', 'ƒ§', ',', 'ƒ†', 'a', 'c', 'e', 'n', 't', 'o', 's', 'ƒ†', '√É', '¬°', '√É', '¬©', '√É', '≈É', '√É', '¬≥', '√É', '¬∫']\n",
            "¬ø<unk> aparece? False\n",
            "DECODE: Byte-level BPE: Ê±âÂ≠ó, emojis üôÇ, acentos √°√©√≠√≥√∫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 5 (comparaci√≥n subword)\n",
        "**Objetivo**: ver c√≥mo cambia la longitud $T$ seg√∫n el tokenizador.\n",
        "\n",
        "**TODO**:\n",
        "1) Define un texto con una palabra muy rara + emojis\n",
        "2) Tokeniza con BPE / WordPiece / Unigram / Byte-level BPE\n",
        "3) Compara el n√∫mero de tokens (longitud)\n",
        "\n",
        "> Pista: la longitud influye en coste de c√≥mputo (Transformers ~ $O(T^2)$ en atenci√≥n)."
      ],
      "metadata": {
        "id": "pMY2ZDOTK198"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Embeddings posicionales (intuici√≥n)\n",
        "En Transformers, la atenci√≥n no sabe el orden por s√≠ misma.\n",
        "Se suma un vector posicional $p_t$ al embedding del token:\n",
        "$$x_t = e_{i_t} + p_t.$$\n",
        "\n",
        "Aqu√≠ implementamos **positional embeddings aprendibles** (m√°s simple que sinusoidales)."
      ],
      "metadata": {
        "id": "MT4nIzW2K-gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_len: int, d_model: int):\n",
        "        super().__init__()\n",
        "        self.pos = nn.Embedding(max_len, d_model)\n",
        "    def forward(self, x):\n",
        "        B,T = x.shape\n",
        "        positions = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
        "        return self.pos(positions)\n",
        "\n",
        "max_len = 32\n",
        "pos_emb = TinyPositionalEmbedding(max_len=max_len, d_model=d_model).to(device)\n",
        "\n",
        "x = x_pad.to(device)\n",
        "tok_vecs = emb(x)\n",
        "pos_vecs = pos_emb(x)\n",
        "x_in = tok_vecs + pos_vecs\n",
        "print('tok_vecs:', tok_vecs.shape)\n",
        "print('pos_vecs:', pos_vecs.shape)\n",
        "print('input final:', x_in.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL7FKGhzK9oY",
        "outputId": "db15a2c7-ad49-4266-8bed-35bb730241ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tok_vecs: torch.Size([4, 8, 16])\n",
            "pos_vecs: torch.Size([4, 8, 16])\n",
            "input final: torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 6 (posiciones)\n",
        "**TODO**:\n",
        "1) Crea dos secuencias con los mismos tokens pero en orden distinto.\n",
        "2) Sin posicional: embeddings token a token son los mismos.\n",
        "3) Con posicional: la suma cambia (por la posici√≥n).\n",
        "\n",
        "Esto ilustra por qu√© los Transformers necesitan informaci√≥n posicional (Que se explicar√° el d√≠a siguiente)."
      ],
      "metadata": {
        "id": "zYLe_SYKLJ6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes Recurrentes (RNN, LSTM, GRU) para PLN\n",
        "Esta secci√≥n corresponde con la secci√≥n **‚ÄúLas redes recurrentes para el procesamiento del lenguaje natural‚Äù**. de la teor√≠a.\n",
        "\n",
        "Donde vamos a explicar algunos detalles y teoria a nivel de c√≥digo:\n",
        "1) Repaso: tipos de datos y por qu√© MLP/CNN no son ideales para secuencias largas\n",
        "2) Qu√© es una RNN (c√©lula, estado, desenrollado)\n",
        "3) **BPTT** en PyTorch (autograd)\n",
        "4) **Vanishing/Exploding gradients** con un experimento controlado\n",
        "5) Soluciones: **gradient clipping** y **truncated BPTT** (`detach()`)\n",
        "6) LSTM y GRU: intuici√≥n, compuertas, y clasificaci√≥n many-to-one\n",
        "\n",
        "Hay **ejercicios intercalados (TODO)**."
      ],
      "metadata": {
        "id": "-bLY8DLpLZUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "miR4_NzQQJo1",
        "outputId": "32c95ff9-5c64-445e-d7f2-395ac7c1a37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos lo de antes, establecer semillas para reproducibilidad\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "print('Seed set with value ', 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs5r_5hCQLLg",
        "outputId": "e1d810aa-bb3a-4655-ae32-db23692c0403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set with value  42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Repaso: tipos de datos ‚Üí sesgos inductivos\n",
        "- **Tabular**: vector sin orden ‚Üí MLP\n",
        "- **Espacial (2D)**: localidad/traslaci√≥n ‚Üí CNN\n",
        "- **Secuencial**: orden + dependencia temporal ‚Üí RNN/Transformer\n",
        "\n",
        "### Por qu√© MLP no es ideal en PLN\n",
        "- Longitud fija\n",
        "- No modela orden\n",
        "- No comparte par√°metros por posici√≥n\n",
        "\n",
        "### Por qu√© CNN mejora pero sigue limitada\n",
        "- Capta n-gramas (localidad)\n",
        "- Paralelizable\n",
        "- Pero el **contexto efectivo** crece lento (receptive field)\n",
        "- Dependencias largas son dif√≠ciles de capturar de forma natural"
      ],
      "metadata": {
        "id": "l-MTmiepQbEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) ¬øQu√© es una RNN?\n",
        "Una RNN es una **misma c√©lula** aplicada repetidamente:\n",
        "\n",
        "$$h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)$$\n",
        "\n",
        "- $x_t$: entrada en el tiempo t\n",
        "- $h_{t-1}$: memoria previa\n",
        "- $h_t$: nueva memoria\n",
        "\n",
        "La 'profundidad' de una RNN es **temporal**: el grafo se desenrolla a lo largo de $T$ pasos.\n",
        "\n",
        "### Many-to-One (clasificaci√≥n)\n",
        "$$(x_1,\\dots,x_T) \\to y$$\n",
        "\n",
        "### One-to-Many (generaci√≥n)\n",
        "$$x \\to (y_1,\\dots,y_T)$$\n",
        "\n",
        "### Many-to-Many (etiquetado)\n",
        "$$(x_1,\\dots,x_T) \\to (y_1,\\dots,y_T)$$"
      ],
      "metadata": {
        "id": "QV95OovqQf4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Implementar una RNN desde cero (c√©lula + desenrollado)\n",
        "Construimos una c√©lula RNN m√≠nima y la aplicamos en un bucle.\n",
        "\n",
        "Esto ayuda a ver:\n",
        "- una sola c√©lula reutilizada\n",
        "- el estado conecta pasos\n",
        "- autograd har√° BPTT al final"
      ],
      "metadata": {
        "id": "DraNQTp8ROz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Una celda RNN \"b√°sica\" (tipo Elman).\n",
        "    Actualiza el estado oculto h_t usando:\n",
        "        h_t = tanh(Wx * x_t + Wh * h_{t-1})\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Proyecci√≥n de la entrada x_t (D -> H) con bias\n",
        "        self.Wx = nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "\n",
        "        # Proyecci√≥n del estado anterior h_{t-1} (H -> H) sin bias\n",
        "        # (muchas implementaciones ponen el bias solo en Wx)\n",
        "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        \"\"\"\n",
        "        x_t:   (B, D)  entrada en el tiempo t\n",
        "        h_prev:(B, H)  estado oculto anterior\n",
        "        devuelve:\n",
        "        h_t:   (B, H)  nuevo estado oculto\n",
        "        \"\"\"\n",
        "        # Suma de contribuci√≥n de la entrada y del estado anterior\n",
        "        preact = self.Wx(x_t) + self.Wh(h_prev)\n",
        "\n",
        "        # No linealidad t√≠pica de RNN simple\n",
        "        h_t = torch.tanh(preact)\n",
        "        return h_t\n",
        "\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    RNN que procesa una secuencia completa.\n",
        "    Recorre T pasos, guarda todos los estados ocultos, y produce logits\n",
        "    a partir del √∫ltimo estado (many-to-one).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # La celda recurrente (actualiza h en cada t)\n",
        "        self.cell = SimpleRNNCell(input_dim, hidden_dim)\n",
        "\n",
        "        # Capa final para convertir el √∫ltimo estado oculto en salida/clases\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        \"\"\"\n",
        "        x:  (B, T, D) batch de secuencias\n",
        "            B = batch size\n",
        "            T = longitud temporal\n",
        "            D = dimensi√≥n de entrada (input_dim)\n",
        "\n",
        "        h0: (B, H) estado inicial opcional\n",
        "        devuelve:\n",
        "        logits: (B, output_dim) salida basada en el √∫ltimo estado\n",
        "        hs:     (B, T, H) todos los estados ocultos a lo largo del tiempo\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        # Si no dan h0, inicializamos h a ceros (estado oculto inicial)\n",
        "        if h0 is None:\n",
        "            H = self.cell.Wh.in_features  # hidden_dim (entrada de Wh)\n",
        "            h = torch.zeros(B, H, device=x.device)\n",
        "        else:\n",
        "            h = h0\n",
        "\n",
        "        hs = []  # aqu√≠ guardaremos cada h_t\n",
        "\n",
        "        # Recorremos la secuencia paso a paso\n",
        "        for t in range(T):\n",
        "            # Tomamos la entrada del tiempo t: (B, D)\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Actualizamos el estado oculto: (B, H)\n",
        "            h = self.cell(x_t, h)\n",
        "\n",
        "            # Guardamos este estado\n",
        "            hs.append(h)\n",
        "\n",
        "        # Many-to-one: usamos SOLO el √∫ltimo estado oculto para predecir\n",
        "        logits = self.fc(hs[-1])  # (B, output_dim)\n",
        "\n",
        "        # Apilamos la lista hs -> tensor (B, T, H)\n",
        "        hs = torch.stack(hs, dim=1)\n",
        "\n",
        "        return logits, hs\n",
        "\n",
        "\n",
        "print(\"SimpleRNN ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBTHV89aQarb",
        "outputId": "951fd59b-c5db-4763-addf-ef1882384be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleRNN ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset de juguete: paridad (depende de todo el pasado)\n",
        "Dada una secuencia de bits, predecir si la suma de 1s es **par** o **impar**.\n",
        "Esto obliga a la red a mantener una memoria a lo largo de T pasos."
      ],
      "metadata": {
        "id": "cA5shLEQR1iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParityDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset para el problema de paridad.\n",
        "    Cada muestra es una secuencia binaria y la etiqueta indica\n",
        "    si el n√∫mero total de unos es par (0) o impar (1).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples=2000, seq_len=20):\n",
        "        self.n = n_samples   # n√∫mero total de ejemplos\n",
        "        self.T = seq_len    # longitud de cada secuencia\n",
        "\n",
        "    def __len__(self):\n",
        "        # N√∫mero de muestras del dataset\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Genera una secuencia aleatoria de 0s y 1s\n",
        "        # x: (T,)\n",
        "        x = torch.randint(0, 2, (self.T,))\n",
        "\n",
        "        # Etiqueta: paridad de la suma\n",
        "        # sum % 2 = 0 ‚Üí par, 1 ‚Üí impar\n",
        "        y = int(x.sum().item() % 2)\n",
        "\n",
        "        # Convertimos a one-hot para usarlo como entrada a la red\n",
        "        # (T,) ‚Üí (T, 2)\n",
        "        x_oh = F.one_hot(x, num_classes=2).float()\n",
        "\n",
        "        return x_oh, y\n",
        "\n",
        "\n",
        "def make_loader(seq_len=20, batch=64):\n",
        "    \"\"\"\n",
        "    Crea un DataLoader listo para entrenar.\n",
        "    \"\"\"\n",
        "    ds = ParityDataset(n_samples=2000, seq_len=seq_len)\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "\n",
        "# Probamos el loader\n",
        "dl = make_loader(seq_len=20, batch=64)\n",
        "\n",
        "# Sacamos un batch\n",
        "x, y = next(iter(dl))\n",
        "\n",
        "print('x:', x.shape)        # (B, T, 2)\n",
        "print('y:', y.shape)        # (B,)\n",
        "print('x[0]', x[0].tolist())\n",
        "print('y[:10]:', y[:10].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijaohEiORxx-",
        "outputId": "e0ed7124-1214-4750-9038-e55078748026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: torch.Size([64, 20, 2])\n",
            "y: torch.Size([64])\n",
            "x[0] [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
            "y[:10]: [0, 1, 0, 0, 1, 1, 0, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1 (desenrollado)\n",
        "**TODO**:\n",
        "1) Ejecuta el modelo una vez\n",
        "2) Imprime shapes de `logits` y `hs`\n",
        "3) Explica qu√© representa `hs[:, t, :]`"
      ],
      "metadata": {
        "id": "wlxwBiGAVOSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Entrenamiento y BPTT (autograd)\n",
        "En PyTorch, `loss.backward()` hace BPTT autom√°ticamente.\n",
        "Inspeccionamos loss y accuracy."
      ],
      "metadata": {
        "id": "B06ULMnHV7cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_parity(model, dl, epochs=10, lr=1e-3, clip=None):\n",
        "    # Ponemos el modelo en modo entrenamiento:\n",
        "    # activa dropout/batchnorm (si existieran) y prepara el comportamiento correcto\n",
        "    model.train()\n",
        "\n",
        "    # Optimizador: Adam actualiza los par√°metros del modelo usando los gradientes\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Funci√≥n de p√©rdida para clasificaci√≥n multiclase:\n",
        "    # espera logits de shape (B, C) y etiquetas y de shape (B,) con valores 0..C-1\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Bucle de √©pocas (pasadas completas por el DataLoader)\n",
        "    for ep in range(1, epochs + 1):\n",
        "        total_loss, total_acc, n = 0.0, 0.0, 0  # acumuladores para m√©tricas\n",
        "\n",
        "        # Iteramos por mini-batches\n",
        "        for x, y in dl:\n",
        "            # x: (B, T, 2)  secuencia one-hot\n",
        "            # y: (B,)       etiqueta (0=par, 1=impar)\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # 1) Resetear gradientes acumulados del paso anterior\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # 2) Forward: obtenemos los logits (B, 2)\n",
        "            #    El modelo devuelve (logits, hs). Aqu√≠ hs no lo usamos.\n",
        "            logits, _ = model(x)\n",
        "\n",
        "            # 3) Loss: CrossEntropy incluye softmax internamente\n",
        "            loss = crit(logits, y)\n",
        "\n",
        "            # 4) Backward: calcula gradientes (BPTT: backprop through time)\n",
        "            loss.backward()\n",
        "\n",
        "            # (Opcional) Clipping de gradientes para estabilizar entrenamiento en RNNs\n",
        "            # Evita explosi√≥n de gradientes\n",
        "            if clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # 5) Paso del optimizador: actualiza los par√°metros con los gradientes\n",
        "            opt.step()\n",
        "\n",
        "            # --- M√©tricas ---\n",
        "            # loss promedio ponderado por tama√±o de batch\n",
        "            total_loss += loss.item() * y.size(0)\n",
        "\n",
        "            # accuracy: comparamos clase predicha (argmax) contra y\n",
        "            preds = logits.argmax(dim=-1)  # (B,)\n",
        "            total_acc += (preds == y).float().sum().item()\n",
        "\n",
        "            # contador total de ejemplos vistos\n",
        "            n += y.size(0)\n",
        "\n",
        "        # Log cada 2 √©pocas (y tambi√©n en la primera)\n",
        "        if ep % 2 == 0 or ep == 1:\n",
        "            print(f'ep {ep:02d} | loss {total_loss/n:.4f} | acc {total_acc/n:.3f}')\n",
        "\n",
        "\n",
        "# Creamos y entrenamos\n",
        "model = SimpleRNN(input_dim=2, hidden_dim=32, output_dim=2).to(device)\n",
        "train_parity(model, dl, epochs=10, lr=1e-3, clip=None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9uIUkVvV7Ke",
        "outputId": "df52793e-2067-4ba3-83c0-4f7930e322ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 01 | loss 0.6974 | acc 0.488\n",
            "ep 02 | loss 0.6961 | acc 0.495\n",
            "ep 04 | loss 0.6954 | acc 0.476\n",
            "ep 06 | loss 0.6942 | acc 0.521\n",
            "ep 08 | loss 0.6944 | acc 0.500\n",
            "ep 10 | loss 0.6939 | acc 0.491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qu√© significa ‚ÄúBPTT‚Äù aqu√≠\n",
        "\n",
        "Tu RNN guarda la cadena de operaciones para todos los timesteps t=0..T-1.\n",
        "Cuando haces loss.backward(), PyTorch propaga gradientes desde el √∫ltimo estado hacia atr√°s por toda la secuencia."
      ],
      "metadata": {
        "id": "F6XDXe_3WPcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 2 (gradientes)\n",
        "**TODO**:\n",
        "1) Haz forward+backward con un batch\n",
        "2) Imprime norma de gradientes de `Wx` y `Wh`\n",
        "3) Explica por qu√© `Wh` es cr√≠tico en dependencias largas"
      ],
      "metadata": {
        "id": "iecbtGOhWJW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Vanishing/Exploding gradient: experimento controlado\n",
        "Simplificaci√≥n:\n",
        "$$\\frac{\\partial h_t}{\\partial h_{t-k}} \\approx (W_h)^k$$\n",
        "Si la norma efectiva es <1 ‚Üí desaparece; >1 ‚Üí explota.\n",
        "\n",
        "Aqu√≠ definimos: $h_t = \\tanh(\\alpha h_{t-1})$ y medimos $\\|\\partial L/\\partial h_0\\|$."
      ],
      "metadata": {
        "id": "eX8ZaXKtWqnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ControlledRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Celda RNN artificial MUY simple.\n",
        "    No tiene pesos aprendibles: solo controla cu√°nto del estado anterior\n",
        "    se propaga al siguiente paso mediante el escalar alpha.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int, alpha: float):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha          # factor de escala (controla estabilidad)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, h_prev):\n",
        "        # Regla de transici√≥n:\n",
        "        # h_t = tanh(alpha * h_{t-1})\n",
        "        return torch.tanh(self.alpha * h_prev)\n",
        "\n",
        "def grad_through_time(alpha: float, T: int = 50, hidden_dim: int = 64):\n",
        "    # Creamos la celda con un alpha fijo\n",
        "    cell = ControlledRNNCell(hidden_dim, alpha).to(device)\n",
        "\n",
        "    # Estado inicial h0, con gradiente activado\n",
        "    # shape: (1, hidden_dim)\n",
        "    h0 = torch.randn(1, hidden_dim, device=device, requires_grad=True)\n",
        "\n",
        "    # Inicializamos el estado\n",
        "    h = h0\n",
        "\n",
        "    # Avanzamos T pasos en el tiempo (sin entrada externa)\n",
        "    for _ in range(T):\n",
        "        h = cell(h)\n",
        "\n",
        "    # Definimos una p√©rdida artificial: suma de todas las componentes finales\n",
        "    loss = h.sum()\n",
        "\n",
        "    # Backpropagation Through Time\n",
        "    loss.backward()\n",
        "\n",
        "    # Devolvemos la norma del gradiente respecto al estado inicial\n",
        "    return float(h0.grad.norm().item())\n",
        "\n",
        "for a in [0.5, 0.9, 1.0, 1.1, 1.5]:\n",
        "    g = grad_through_time(alpha=a, T=60)\n",
        "    print(f'alpha={a:3.1f} -> ||dL/dh0|| = {g:.6e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-G3T8X_WrPq",
        "outputId": "15ece7e1-d691-4460-c660-48df85778942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha=0.5 -> ||dL/dh0|| = 5.507782e-18\n",
            "alpha=0.9 -> ||dL/dh0|| = 8.623534e-03\n",
            "alpha=1.0 -> ||dL/dh0|| = 3.017680e+00\n",
            "alpha=1.1 -> ||dL/dh0|| = 6.628270e+00\n",
            "alpha=1.5 -> ||dL/dh0|| = 1.254204e-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 3 (efecto de T)\n",
        "**TODO**: prueba T=10,30,100 en alpha=0.9 y alpha=1.1 y explica el patr√≥n."
      ],
      "metadata": {
        "id": "7pMDEFbCW53Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Soluci√≥n para exploding: Gradient clipping\n",
        "Mostramos entrenamiento con y sin clipping en secuencias m√°s largas."
      ],
      "metadata": {
        "id": "Pllgna7FXB9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader con secuencias m√°s largas (T=60).\n",
        "# Esto hace el problema m√°s dif√≠cil para una RNN simple y aumenta el riesgo\n",
        "# de vanishing/exploding gradients.\n",
        "dl_long = make_loader(seq_len=60, batch=64)\n",
        "\n",
        "# Dos modelos id√©nticos para comparar de forma justa:\n",
        "# - uno sin clipping\n",
        "# - otro con clipping\n",
        "model_no_clip = SimpleRNN(input_dim=2, hidden_dim=64, output_dim=2).to(device)\n",
        "model_clip    = SimpleRNN(input_dim=2, hidden_dim=64, output_dim=2).to(device)\n",
        "\n",
        "print('--- Sin clipping ---')\n",
        "# Entrenamos 6 √©pocas con lr=3e-3 (algo m√°s alto que 1e-3),\n",
        "# y SIN recortar gradientes.\n",
        "train_parity(model_no_clip, dl_long, epochs=6, lr=3e-3, clip=None)\n",
        "\n",
        "print('\\n--- Con clipping (1.0) ---')\n",
        "# Entrenamos 6 √©pocas con el mismo lr,\n",
        "# pero ahora recortando la norma global de gradientes a 1.0.\n",
        "# Esto ayuda a evitar pasos gigantes cuando los gradientes explotan.\n",
        "train_parity(model_clip, dl_long, epochs=6, lr=3e-3, clip=1.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgO4VLpLXFOp",
        "outputId": "ac62c9ec-c527-44a6-c327-c289b0a05534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sin clipping ---\n",
            "ep 01 | loss 0.6984 | acc 0.510\n",
            "ep 02 | loss 0.7032 | acc 0.490\n",
            "ep 04 | loss 0.6979 | acc 0.494\n",
            "ep 06 | loss 0.6960 | acc 0.492\n",
            "\n",
            "--- Con clipping (1.0) ---\n",
            "ep 01 | loss 0.6985 | acc 0.520\n",
            "ep 02 | loss 0.7064 | acc 0.504\n",
            "ep 04 | loss 0.6971 | acc 0.512\n",
            "ep 06 | loss 0.6948 | acc 0.523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Truncated BPTT + detach()\n",
        "Procesamos la secuencia por chunks y cortamos el grafo para limitar cu√°nto atr√°s viaja el gradiente."
      ],
      "metadata": {
        "id": "wTZ_kMDyXQJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncated_bptt_last_chunk_grad(T=80, chunk=20, hidden_dim=64, alpha=0.9):\n",
        "    cell = ControlledRNNCell(hidden_dim, alpha).to(device)\n",
        "\n",
        "    # Estado inicial (leaf)\n",
        "    h = torch.randn(1, hidden_dim, device=device, requires_grad=True)\n",
        "\n",
        "    # Iremos guardando aqu√≠ el \"inicio del √∫ltimo chunk\" como leaf\n",
        "    h_start_last_leaf = None\n",
        "\n",
        "    # El √∫ltimo chunk empieza en el paso: T - chunk (0-indexed)\n",
        "    last_start_step = T - chunk  # ej: T=80, chunk=20 -> empieza en t=60\n",
        "\n",
        "    for t in range(T):\n",
        "        # Si estamos justo al inicio del √∫ltimo chunk,\n",
        "        # hacemos detach para cortar el grafo ANTES de entrar al √∫ltimo chunk\n",
        "        # y convertimos h en un leaf con requires_grad=True\n",
        "        if t == last_start_step:\n",
        "            h = h.detach().requires_grad_(True)\n",
        "            h_start_last_leaf = h  # este S√ç es leaf\n",
        "\n",
        "        # Avanzamos un paso\n",
        "        h = cell(h)\n",
        "\n",
        "        # Truncamiento normal cada 'chunk' pasos (excepto al final)\n",
        "        if (t + 1) % chunk == 0 and (t + 1) != T:\n",
        "            h = h.detach().requires_grad_(True)\n",
        "\n",
        "    loss = h.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    # Si chunk == T, last_start_step = 0, as√≠ que h_start_last_leaf se define bien\n",
        "    return float(h_start_last_leaf.grad.norm().item())\n",
        "\n",
        "\n",
        "for chunk in [10, 20, 40, 80]:\n",
        "    g = truncated_bptt_last_chunk_grad(T=80, chunk=chunk, alpha=0.9)\n",
        "    print(f'chunk={chunk:3d} -> ||dL/dh_start_last|| = {g:.6e}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1tAbA7SXNyS",
        "outputId": "bd845a43-1bb1-4f88-fc34-e9f6158e074a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunk= 10 -> ||dL/dh_start_last|| = 2.789426e+00\n",
            "chunk= 20 -> ||dL/dh_start_last|| = 9.726100e-01\n",
            "chunk= 40 -> ||dL/dh_start_last|| = 1.182283e-01\n",
            "chunk= 80 -> ||dL/dh_start_last|| = 8.430752e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) LSTM y GRU: clasificaci√≥n many-to-one\n",
        "Ahora pasamos al caso PLN t√≠pico:\n",
        "**Embedding ‚Üí (LSTM/GRU) ‚Üí √∫ltimo estado ‚Üí Linear ‚Üí clase**\n",
        "\n",
        "Usamos tokenizaci√≥n whitespace para no mezclar temas."
      ],
      "metadata": {
        "id": "fMmLOjPaXya_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "@dataclass\n",
        "class Vocab:\n",
        "    stoi: dict   # string -> id\n",
        "    itos: list   # id -> string\n",
        "    pad: str = '<pad>'\n",
        "    unk: str = '<unk>'\n",
        "    bos: str = '<bos>'\n",
        "    eos: str = '<eos>'\n",
        "\n",
        "    # IDs de tokens especiales\n",
        "    @property\n",
        "    def pad_id(self): return self.stoi[self.pad]\n",
        "    @property\n",
        "    def unk_id(self): return self.stoi[self.unk]\n",
        "    @property\n",
        "    def bos_id(self): return self.stoi[self.bos]\n",
        "    @property\n",
        "    def eos_id(self): return self.stoi[self.eos]\n",
        "\n",
        "def build_vocab(texts, min_freq=1):\n",
        "    # Cuenta palabras en todos los textos\n",
        "    c = Counter()\n",
        "    for t in texts:\n",
        "        c.update(t.lower().split())\n",
        "\n",
        "    # Tokens especiales primero (quedan con ids 0..3)\n",
        "    specials = ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "\n",
        "    # A√±adimos el resto de palabras que aparecen al menos min_freq veces\n",
        "    itos = specials + [w for w, f in c.items() if f >= min_freq and w not in specials]\n",
        "    stoi = {w: i for i, w in enumerate(itos)}\n",
        "\n",
        "    return Vocab(stoi, itos)\n",
        "\n",
        "def encode(v: Vocab, text: str, add_bos_eos=True):\n",
        "    # Tokenizaci√≥n simple por espacios (para demo)\n",
        "    toks = text.lower().split()\n",
        "\n",
        "    # Convertimos tokens a ids (si no est√°, usamos unk_id)\n",
        "    ids = [v.stoi.get(w, v.unk_id) for w in toks]\n",
        "\n",
        "    # A√±adimos tokens de inicio y fin si se pide\n",
        "    if add_bos_eos:\n",
        "        ids = [v.bos_id] + ids + [v.eos_id]\n",
        "    return ids\n",
        "\n",
        "def pad_batch(seqs, pad_id):\n",
        "    # Longitudes reales de cada secuencia\n",
        "    lens = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "\n",
        "    # Longitud m√°xima en este batch\n",
        "    T = int(lens.max().item())\n",
        "\n",
        "    # Matriz (B, T) rellena con pad_id\n",
        "    out = torch.full((len(seqs), T), pad_id, dtype=torch.long)\n",
        "\n",
        "    # Copiamos cada secuencia al principio (padding a la derecha)\n",
        "    for i, s in enumerate(seqs):\n",
        "        out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "\n",
        "    return out, lens\n",
        "\n",
        "# Datos de ejemplo\n",
        "texts = [\n",
        "    'el perro que persiguio al gato estaba cansado',\n",
        "    'me gusta la pizza',\n",
        "    'pytorch entrena redes recurrentes',\n",
        "    'hoy hace sol en la playa',\n",
        "    'la rnn mantiene memoria del pasado',\n",
        "    'cnn capta patrones locales',\n",
        "    'me encanta aprender nlp',\n",
        "    'el banco del parque es verde',\n",
        "    'el banco central sube tipos',\n",
        "    'las lstm usan compuertas'\n",
        "]\n",
        "labels = [1,1,0,1,0,0,0,1,0,0]\n",
        "\n",
        "# Construimos vocabulario con estos textos\n",
        "v = build_vocab(texts)\n",
        "\n",
        "class TextCls(Dataset):\n",
        "    # Dataset que devuelve (secuencia_ids, label)\n",
        "    def __init__(self, texts, labels, v):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.v = v\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # ids = lista de ints (longitud variable)\n",
        "        ids = encode(self.v, self.texts[i], add_bos_eos=True)\n",
        "        y = int(self.labels[i])\n",
        "        return ids, y\n",
        "\n",
        "def collate(batch):\n",
        "    # batch es una lista de elementos devueltos por el Dataset\n",
        "    # cada uno: (lista_ids, y)\n",
        "    seqs, ys = zip(*batch)\n",
        "\n",
        "    # Pad a la longitud m√°xima del batch\n",
        "    x, l = pad_batch(list(seqs), v.pad_id)\n",
        "\n",
        "    # Convertimos labels a tensor\n",
        "    y = torch.tensor(ys, dtype=torch.long)\n",
        "    return x, l, y\n",
        "\n",
        "# DataLoader con collate_fn para padding din√°mico por batch\n",
        "dl_text = DataLoader(\n",
        "    TextCls(texts, labels, v),\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate\n",
        ")\n",
        "\n",
        "# Probamos un batch\n",
        "xb, lb, yb = next(iter(dl_text))\n",
        "print('xb:', xb.shape, 'lengths:', lb.tolist(), 'y:', yb.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM1sY6wdXzz5",
        "outputId": "74e3191c-2b02-4841-a1e9-a2b4d6482a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xb: torch.Size([4, 10]) lengths: [6, 10, 7, 6] y: [0, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, hidden=128, num_classes=2, pad_id=0):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding: convierte ids (0..vocab_size-1) en vectores de tama√±o d_model.\n",
        "        # padding_idx=pad_id hace que:\n",
        "        # - la fila del PAD se mantenga fija (no se actualiza)\n",
        "        # - su gradiente sea 0\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "\n",
        "        # LSTM (batch_first=True => entrada (B, T, d_model))\n",
        "        self.lstm = nn.LSTM(d_model, hidden, batch_first=True)\n",
        "\n",
        "        # Capa final de clasificaci√≥n\n",
        "        self.fc = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        x:       (B, T) ids con padding\n",
        "        lengths: (B,) longitudes reales (sin contar el padding)\n",
        "        devuelve:\n",
        "        logits:  (B, num_classes)\n",
        "        \"\"\"\n",
        "        # (B, T) -> (B, T, d_model)\n",
        "        e = self.emb(x)\n",
        "\n",
        "        # Empaquetado para ignorar pasos PAD.\n",
        "        # enforce_sorted=False permite que el batch no est√© ordenado por longitud.\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pasamos el packed por la LSTM.\n",
        "        # Para LSTM, la salida final es (hT, cT):\n",
        "        # hT: (num_layers * num_directions, B, hidden)\n",
        "        # cT: (num_layers * num_directions, B, hidden)\n",
        "        _, (hT, cT) = self.lstm(packed)\n",
        "\n",
        "        # Usamos el hidden final de la √∫ltima capa: hT[-1] -> (B, hidden)\n",
        "        # y lo proyectamos a clases: (B, 2)\n",
        "        logits = self.fc(hT[-1])\n",
        "        return logits\n",
        "\n",
        "\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, hidden=128, num_classes=2, pad_id=0):\n",
        "        super().__init__()\n",
        "\n",
        "        # Igual: embedding con padding_idx\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "\n",
        "        # GRU (similar a LSTM pero sin cT)\n",
        "        self.gru = nn.GRU(d_model, hidden, batch_first=True)\n",
        "\n",
        "        # Clasificador final\n",
        "        self.fc = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # (B, T) -> (B, T, d_model)\n",
        "        e = self.emb(x)\n",
        "\n",
        "        # Empaquetado para ignorar padding\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Para GRU, devuelve:\n",
        "        # hT: (num_layers * num_directions, B, hidden)\n",
        "        _, hT = self.gru(packed)\n",
        "\n",
        "        # √öltima capa: (B, hidden) -> logits (B, 2)\n",
        "        logits = self.fc(hT[-1])\n",
        "        return logits\n",
        "\n",
        "\n",
        "def train_cls(model, dl, epochs=25, lr=2e-3, clip=1.0):\n",
        "    # Mandamos el modelo al device (CPU/GPU)\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizador\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Loss para clasificaci√≥n\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        tot_loss, tot_acc, n = 0.0, 0.0, 0\n",
        "\n",
        "        for x, l, y in dl:\n",
        "            # x: (B, T)  ids padded\n",
        "            # l: (B,)    longitudes reales\n",
        "            # y: (B,)    etiquetas\n",
        "            x, l, y = x.to(device), l.to(device), y.to(device)\n",
        "\n",
        "            # Reset gradientes\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Forward: logits (B, 2)\n",
        "            logits = model(x, l)\n",
        "\n",
        "            # Loss\n",
        "            loss = crit(logits, y)\n",
        "\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "\n",
        "            # Clipping: evita exploding gradients\n",
        "            if clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # Paso del optimizador\n",
        "            opt.step()\n",
        "\n",
        "            # M√©tricas\n",
        "            tot_loss += loss.item() * y.size(0)\n",
        "            tot_acc  += (logits.argmax(-1) == y).float().sum().item()\n",
        "            n += y.size(0)\n",
        "\n",
        "        # Print cada 5 epochs (y la primera)\n",
        "        if ep % 5 == 0 or ep == 1:\n",
        "            print(f'ep {ep:02d} | loss {tot_loss/n:.4f} | acc {tot_acc/n:.3f}')\n",
        "\n",
        "\n",
        "print('--- LSTM ---')\n",
        "train_cls(LSTMClassifier(len(v.itos), pad_id=v.pad_id), dl_text)\n",
        "\n",
        "print('\\n--- GRU ---')\n",
        "train_cls(GRUClassifier(len(v.itos), pad_id=v.pad_id), dl_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NT8ookFX_aV",
        "outputId": "0e4a83ce-2be1-4ada-98ca-6e6b2f65e9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LSTM ---\n",
            "ep 01 | loss 0.7291 | acc 0.400\n",
            "ep 05 | loss 0.2839 | acc 1.000\n",
            "ep 10 | loss 0.0014 | acc 1.000\n",
            "ep 15 | loss 0.0002 | acc 1.000\n",
            "ep 20 | loss 0.0001 | acc 1.000\n",
            "ep 25 | loss 0.0001 | acc 1.000\n",
            "\n",
            "--- GRU ---\n",
            "ep 01 | loss 0.7211 | acc 0.600\n",
            "ep 05 | loss 0.1435 | acc 1.000\n",
            "ep 10 | loss 0.0007 | acc 1.000\n",
            "ep 15 | loss 0.0001 | acc 1.000\n",
            "ep 20 | loss 0.0001 | acc 1.000\n",
            "ep 25 | loss 0.0001 | acc 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 4 (many-to-one en texto)\n",
        "**TODO**:\n",
        "1) A√±ade 5 frases nuevas por clase\n",
        "2) Re-entrena y compara LSTM vs GRU\n",
        "3) Quita `pack_padded_sequence` y observa cambios\n",
        "4) Explica por qu√© el padding puede sesgar el resumen si no se maneja\n"
      ],
      "metadata": {
        "id": "QJlvVAdCYrL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El padding puede sesgar el resumen porque, si no se maneja expl√≠citamente, la RNN sigue actualizando su estado oculto durante los timesteps <pad>.\n",
        "En un esquema many-to-one que toma el √∫ltimo estado del batch, ese estado puede corresponder a padding y no al final real de la secuencia.\n",
        "Aunque el embedding del PAD est√© congelado, la din√°mica recurrente sigue modificando el estado y el modelo puede aprender a usar la cantidad de padding (longitud) como pista espuria.\n",
        "En datasets peque√±os esto puede no afectar la accuracy, pero en problemas reales perjudica la generalizaci√≥n."
      ],
      "metadata": {
        "id": "kHW5qhxlZNju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos de lenguaje autorregresivos (RNN/LSTM)\n",
        "Esta secci√≥n implementa y explica, paso a paso:\n",
        "- Qu√© es un **modelo de lenguaje** y por qu√© asigna probabilidades a secuencias\n",
        "- Factorizaci√≥n **autorregresiva**: $P(w_{1:T}) = \\prod_t P(w_t\\mid w_{<t})$\n",
        "- Entrenamiento con **teacher forcing** (shift input/target)\n",
        "- Implementaci√≥n de un **LSTM Language Model** en PyTorch\n",
        "- **Generaci√≥n** token a token (argmax / sampling / temperature)\n",
        "- Evaluaci√≥n con **perplejidad (PPL)**\n",
        "\n",
        "¬°Incluye ejercicios intercalados (TODO)!"
      ],
      "metadata": {
        "id": "9t5h9K4mc-uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tnbvJ1yAbbEo",
        "outputId": "553b9e91-8b1b-4baf-9834-c1e68cf6b5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "print('Seed set')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPp-43kCdOTq",
        "outputId": "98abb0d6-1f6c-4c50-d53f-27ae61f4670d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) ¬øQu√© es un modelo de lenguaje?\n",
        "Un modelo de lenguaje asigna probabilidades a secuencias:\n",
        "$$P(w_1,\\dots,w_T).$$\n",
        "\n",
        "Intuici√≥n: completa huecos *\"Hoy hace mucho ___\"*.\n",
        "\n",
        "## 2) Autorregresivo\n",
        "Un modelo autorregresivo predice la siguiente palabra usando el pasado:\n",
        "$$P(w_1,\\dots,w_T)=\\prod_{t=1}^{T}P(w_t\\mid w_{<t}).$$\n",
        "\n",
        "Esto permite **generaci√≥n**: el modelo se alimenta de sus predicciones.\n",
        "\n",
        "## 3) Preparar datos: tokenizaci√≥n + IDs\n",
        "Para centrarnos en el modelo, usaremos tokenizaci√≥n whitespace y un vocabulario peque√±o.\n",
        "\n",
        "Incluimos tokens especiales:\n",
        "- `<bos>` inicio\n",
        "- `<eos>` fin\n",
        "- `<pad>` padding (si hacemos batches de secuencias)\n",
        "- `<unk>` OOV\n",
        "\n",
        "En un LM autorregresivo entrenamos con:\n",
        "- input: `[w1,...,w(T-1)]`\n",
        "- target: `[w2,...,wT]` (shift)"
      ],
      "metadata": {
        "id": "SwDExKFSdSEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "@dataclass\n",
        "class Vocab:\n",
        "    # Mapa palabra -> id\n",
        "    stoi: Dict[str, int]\n",
        "\n",
        "    # Mapa id -> palabra\n",
        "    itos: List[str]\n",
        "\n",
        "    # Tokens especiales\n",
        "    pad: str = '<pad>'\n",
        "    unk: str = '<unk>'\n",
        "    bos: str = '<bos>'\n",
        "    eos: str = '<eos>'\n",
        "\n",
        "    # Accesos r√°pidos a los ids de los tokens especiales\n",
        "    @property\n",
        "    def pad_id(self): return self.stoi[self.pad]\n",
        "\n",
        "    @property\n",
        "    def unk_id(self): return self.stoi[self.unk]\n",
        "\n",
        "    @property\n",
        "    def bos_id(self): return self.stoi[self.bos]\n",
        "\n",
        "    @property\n",
        "    def eos_id(self): return self.stoi[self.eos]\n",
        "\n",
        "\n",
        "def build_vocab(texts: List[str], min_freq: int = 1) -> Vocab:\n",
        "    \"\"\"\n",
        "    Construye un vocabulario a partir de una lista de textos.\n",
        "    \"\"\"\n",
        "    # Contador de frecuencias de palabras\n",
        "    c = Counter()\n",
        "    for t in texts:\n",
        "        c.update(t.lower().split())\n",
        "\n",
        "    # Tokens especiales (siempre primero)\n",
        "    specials = ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "\n",
        "    # Lista id -> token\n",
        "    # Incluimos palabras que aparecen al menos min_freq veces\n",
        "    itos = specials + [\n",
        "        w for w, f in c.items()\n",
        "        if f >= min_freq and w not in specials\n",
        "    ]\n",
        "\n",
        "    # Diccionario token -> id\n",
        "    stoi = {w: i for i, w in enumerate(itos)}\n",
        "\n",
        "    return Vocab(stoi=stoi, itos=itos)\n",
        "\n",
        "\n",
        "def encode(v: Vocab, text: str, add_bos_eos: bool = True) -> List[int]:\n",
        "    \"\"\"\n",
        "    Convierte un texto en una lista de ids.\n",
        "    \"\"\"\n",
        "    # Tokenizaci√≥n simple\n",
        "    toks = text.lower().split()\n",
        "\n",
        "    # Mapeo palabra -> id (unk si no existe)\n",
        "    ids = [v.stoi.get(w, v.unk_id) for w in toks]\n",
        "\n",
        "    # A√±adimos BOS y EOS si se pide\n",
        "    if add_bos_eos:\n",
        "        ids = [v.bos_id] + ids + [v.eos_id]\n",
        "\n",
        "    return ids\n",
        "\n",
        "\n",
        "def decode(v: Vocab, ids: List[int]) -> str:\n",
        "    \"\"\"\n",
        "    Convierte una lista de ids de vuelta a texto (para debug).\n",
        "    \"\"\"\n",
        "    return ' '.join(\n",
        "        v.itos[i] if 0 <= i < len(v.itos) else '<oov>'\n",
        "        for i in ids\n",
        "    )\n",
        "\n",
        "\n",
        "print('Vocab utils ready')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Btq3OxCdP7p",
        "outputId": "b7ab8097-7451-486f-9251-7bca479c896b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab utils ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'hoy hace mucho frio en madrid',\n",
        "    'hoy hace mucho calor en sevilla',\n",
        "    'el gato duerme en el sofa',\n",
        "    'el perro duerme en el suelo',\n",
        "    'me gusta aprender nlp con pytorch',\n",
        "    'las rnn predicen la siguiente palabra',\n",
        "    'las lstm usan compuertas para recordar',\n",
        "    'los modelos autorregresivos generan texto',\n",
        "    'el gradiente puede desaparecer o explotar',\n",
        "    'hacemos teacher forcing durante el entrenamiento',\n",
        "]\n",
        "\n",
        "vocab = build_vocab(corpus, min_freq=1)\n",
        "print('Vocab size:', len(vocab.itos))\n",
        "print('Ejemplo vocab:', vocab.itos[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC58PJK5dXq5",
        "outputId": "f4278c7d-0c2d-40ba-c198-4c291400b238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 50\n",
            "Ejemplo vocab: ['<pad>', '<unk>', '<bos>', '<eos>', 'hoy', 'hace', 'mucho', 'frio', 'en', 'madrid', 'calor', 'sevilla', 'el', 'gato', 'duerme', 'sofa', 'perro', 'suelo', 'me', 'gusta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1 (factorizaci√≥n)\n",
        "**TODO**: en tus palabras, explica qu√© significa:\n",
        "$$P(w_1,\\dots,w_T)=\\prod_t P(w_t\\mid w_{<t}).$$\n",
        "\n",
        "Luego, responde:\n",
        "1) ¬øPor qu√© esto permite generaci√≥n?\n",
        "2) ¬øQu√© informaci√≥n usa el modelo para predecir $w_t$?\n",
        "\n",
        "> Soluci√≥n esperada: usa contexto previo, palabra a palabra, y al generar reutiliza predicciones como entrada."
      ],
      "metadata": {
        "id": "NjmaHpCwdc7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Dataset autorregresivo (shift)\n",
        "Vamos a convertir el corpus en una secuencia larga de IDs y crear pares (x,y) por ventanas.\n",
        "\n",
        "Esto es t√≠pico para LM con RNN: **truncated BPTT** impl√≠cito por ventanas.\n",
        "\n",
        "- `block_size = T` define cu√°ntos tokens de contexto usamos.\n",
        "- `x = ids[i:i+T]`\n",
        "- `y = ids[i+1:i+T+1]`"
      ],
      "metadata": {
        "id": "AsxBzh7JeTId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista donde acumularemos TODOS los tokens del corpus\n",
        "all_ids = []\n",
        "\n",
        "# Recorremos cada frase del corpus\n",
        "for s in corpus:\n",
        "    # Codificamos la frase:\n",
        "    # texto -> ids, a√±adiendo <bos> y <eos>\n",
        "    ids = encode(vocab, s, add_bos_eos=True)\n",
        "\n",
        "    # A√±adimos esos ids a la lista global (aplanado)\n",
        "    all_ids.extend(ids)\n",
        "\n",
        "# Convertimos la lista a tensor (necesario para PyTorch)\n",
        "all_ids = torch.tensor(all_ids, dtype=torch.long)\n",
        "\n",
        "# N√∫mero total de tokens (incluye <bos> y <eos>)\n",
        "print('Total tokens:', len(all_ids))\n",
        "\n",
        "# Decodificamos algunos tokens para comprobar que todo est√° correcto\n",
        "print('Decode sample:', decode(vocab, all_ids[:12].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCk8r_N7dfJG",
        "outputId": "b3b0ec4a-ad95-4630-a2e1-8780dc865265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 79\n",
            "Decode sample: <bos> hoy hace mucho frio en madrid <eos> <bos> hoy hace mucho\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicios y recurso del uso de RNN como LSTM/GRU en PLN: Modelo de lenguaje (siguiente palabra) con GRU ‚Üí c√°mbialo a LSTM y compara\n",
        "\n",
        "### Objetivo\n",
        "Ahora practicamos un modelo de **lenguaje autorregresivo** (next-token prediction) usando un dataset p√∫blico real:** WikiText-2.**\n",
        "\n",
        "- Entrada: `x = (w1, w2, ..., w_{T-1})`\n",
        "- Target: `y = (w2, w3, ..., w_T)`\n",
        "- La red produce logits **en cada paso temporal**, y entrenamos con `CrossEntropyLoss` token a token.\n",
        "- Se eval√∫a con: **Negative Log-Likelihood (NLL)** y **Perplexity (PPL)**\n",
        "\n",
        "### Dataset:\n",
        "Utilizaremos\n",
        "**WikiText-2 (raw)**: https://huggingface.co/datasets/Salesforce/wikitext (usamores los 2500 primeros textos de train para el entrenamiento y 500 de validaci√≥n por la limitaci√≥n de GPU)\n",
        "\n",
        "### Arquitectura del modelo\n",
        "Embedding ‚Üí RNN (GRU o LSTM) ‚Üí Linear ‚Üí Softmax\n",
        "\n",
        "### Qu√© tienes que hacer (TODO)\n",
        "1. Entrena el modelo base **GRU** (ya dado) y anota `loss` y `perplexity`.\n",
        "2. Duplica el modelo y sustit√∫yelo por **LSTM**.\n",
        "3. Entrena con los **mismos hiperpar√°metros** (epochs, lr, hidden, etc.).\n",
        "4. Compara: ¬ømejora la `perplexity`? ¬øse entrena m√°s estable?\n",
        "\n"
      ],
      "metadata": {
        "id": "0Awx3mHsZWWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# -------------------------\n",
        "# 1) Vocab + encode/decode\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class VocabLM:\n",
        "    stoi: Dict[str, int]\n",
        "    itos: List[str]\n",
        "    pad: str = \"<pad>\"\n",
        "    unk: str = \"<unk>\"\n",
        "    bos: str = \"<bos>\"\n",
        "    eos: str = \"<eos>\"\n",
        "\n",
        "    @property\n",
        "    def pad_id(self): return self.stoi[self.pad]\n",
        "    @property\n",
        "    def unk_id(self): return self.stoi[self.unk]\n",
        "    @property\n",
        "    def bos_id(self): return self.stoi[self.bos]\n",
        "    @property\n",
        "    def eos_id(self): return self.stoi[self.eos]\n",
        "\n",
        "\n",
        "def build_vocab_lm(texts: List[str], min_freq: int = 2, max_vocab: int = 50000) -> VocabLM:\n",
        "    c = Counter()\n",
        "    for t in texts:\n",
        "        c.update(t.lower().split())\n",
        "\n",
        "    specials = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "    # ordenamos por frecuencia (desc) y luego lexicogr√°fico para estabilidad\n",
        "    words = sorted([(w, f) for w, f in c.items() if w not in specials and f >= min_freq],\n",
        "                   key=lambda x: (-x[1], x[0]))\n",
        "    words = [w for w, _ in words][: max(0, max_vocab - len(specials))]\n",
        "\n",
        "    itos = specials + words\n",
        "    stoi = {w: i for i, w in enumerate(itos)}\n",
        "    return VocabLM(stoi=stoi, itos=itos)\n",
        "\n",
        "\n",
        "def encode_lm(v: VocabLM, text: str, add_bos_eos: bool = True) -> List[int]:\n",
        "    toks = text.lower().split()\n",
        "    ids = [v.stoi.get(w, v.unk_id) for w in toks]\n",
        "    if add_bos_eos:\n",
        "        ids = [v.bos_id] + ids + [v.eos_id]\n",
        "    return ids\n",
        "\n",
        "\n",
        "def decode_lm(v: VocabLM, ids: List[int], skip_specials: bool = True) -> str:\n",
        "    specials = {v.pad, v.unk, v.bos, v.eos}\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        w = v.itos[i] if 0 <= i < len(v.itos) else v.unk\n",
        "        if skip_specials and w in specials:\n",
        "            continue\n",
        "        toks.append(w)\n",
        "    return \" \".join(toks)"
      ],
      "metadata": {
        "id": "grqgo3M4aapR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 2) Dataset LM por ventanas\n",
        "# -------------------------\n",
        "class LMDataset(Dataset):\n",
        "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
        "        self.ids = ids\n",
        "        self.block = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.ids) - self.block - 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.ids[idx: idx + self.block]           # (T,)\n",
        "        y = self.ids[idx + 1: idx + self.block + 1]   # (T,)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "rUQtAtPhacVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------\n",
        "# 3) Modelo: Emb + (GRU|LSTM) + Linear\n",
        "# -------------------------\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        emb_dim: int = 128,\n",
        "        hidden_dim: int = 256,\n",
        "        num_layers: int = 1,\n",
        "        rnn_type: str = \"gru\",      # \"gru\" o \"lstm\"\n",
        "        dropout: float = 0.0,\n",
        "        pad_id: int = 0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
        "\n",
        "        rnn_type = rnn_type.lower()\n",
        "        self.rnn_type = rnn_type\n",
        "        if rnn_type == \"gru\":\n",
        "            self.rnn = nn.GRU(\n",
        "                input_size=emb_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"rnn_type debe ser 'gru' o 'lstm'\")\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, h=None):\n",
        "        # x: (B, T)\n",
        "        e = self.emb(x)             # (B, T, E)\n",
        "        out, h = self.rnn(e, h)     # out: (B, T, H)\n",
        "        logits = self.fc(out)       # (B, T, V)\n",
        "        return logits, h\n"
      ],
      "metadata": {
        "id": "XZS5NPM0advM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento autorregresivo (teacher forcing)\n",
        "Entrenamiento t√≠pico:\n",
        "- `x` ya es contexto\n",
        "- `y` es el target\n",
        "- Loss: CrossEntropy sobre todos los pasos\n",
        "\n",
        "Medimos tambi√©n **perplejidad**:\n",
        "$$\\mathrm{PPL} = \\exp(\\text{loss promedio})$$\n",
        "cuando loss es NLL por token."
      ],
      "metadata": {
        "id": "IBenZjK0fFDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo base (GRU) - EJEMPLO\n"
      ],
      "metadata": {
        "id": "ma2_NHjSajXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4) Train/Eval (loss + ppl)\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, dl: DataLoader, device: str) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for x, y in dl:\n",
        "        x, y = x.to(device), y.to(device)              # (B,T)\n",
        "        logits, _ = model(x)                           # (B,T,V)\n",
        "        loss = F.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            y.reshape(-1),\n",
        "            reduction=\"sum\"\n",
        "        )\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += y.numel()\n",
        "\n",
        "    avg_nll = total_loss / max(1, total_tokens)       # NLL por token\n",
        "    ppl = math.exp(min(20, avg_nll))                   # cap para evitar inf\n",
        "    return avg_nll, ppl\n",
        "\n",
        "\n",
        "def train_one_model(\n",
        "    rnn_type: str,\n",
        "    train_dl: DataLoader,\n",
        "    valid_dl: DataLoader,\n",
        "    vocab_size: int,\n",
        "    pad_id: int,\n",
        "    device: str,\n",
        "    emb_dim: int = 128,\n",
        "    hidden_dim: int = 256,\n",
        "    num_layers: int = 1,\n",
        "    dropout: float = 0.0,\n",
        "    lr: float = 2e-3,\n",
        "    epochs: int = 5,\n",
        "    grad_clip: float = 1.0,\n",
        "    seed: int = 123\n",
        "):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    model = RNNLM(\n",
        "        vocab_size=vocab_size,\n",
        "        emb_dim=emb_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        rnn_type=rnn_type,\n",
        "        dropout=dropout,\n",
        "        pad_id=pad_id\n",
        "    ).to(device)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        tokens = 0\n",
        "\n",
        "        for x, y in train_dl:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            logits, _ = model(x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                y.reshape(-1),\n",
        "                reduction=\"mean\"\n",
        "            )\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            if grad_clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            opt.step()\n",
        "\n",
        "            running += loss.item() * y.numel()\n",
        "            tokens += y.numel()\n",
        "\n",
        "        train_nll = running / max(1, tokens)\n",
        "        train_ppl = math.exp(min(20, train_nll))\n",
        "\n",
        "        val_nll, val_ppl = evaluate(model, valid_dl, device)\n",
        "\n",
        "        print(f\"[{rnn_type.upper()}] epoch {ep:02d} | \"\n",
        "              f\"train_nll {train_nll:.4f} ppl {train_ppl:.2f} | \"\n",
        "              f\"valid_nll {val_nll:.4f} ppl {val_ppl:.2f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GQJlofUVarhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 5) Inferencia: generar texto\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def generate_next_words(\n",
        "    model: nn.Module,\n",
        "    vocab: VocabLM,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 20,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: Optional[int] = 50,\n",
        "    device: str = \"cpu\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generaci√≥n autoregresiva:\n",
        "    - Codifica el prompt (con <bos> ... sin <eos> al final)\n",
        "    - Va prediciendo token a token\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # ids iniciales: <bos> + tokens prompt (sin eos)\n",
        "    prompt_ids = [vocab.bos_id] + [vocab.stoi.get(w, vocab.unk_id) for w in prompt.lower().split()]\n",
        "    ids = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
        "\n",
        "    h = None\n",
        "    # \"Warm-up\" pasando todo el prompt para obtener estado oculto\n",
        "    logits, h = model(ids, h)\n",
        "\n",
        "    last_id = ids[:, -1:]  # (1,1)\n",
        "\n",
        "    generated = prompt_ids[:]  # copia\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits, h = model(last_id, h)             # (1,1,V)\n",
        "        next_logits = logits[:, -1, :]            # (1,V)\n",
        "        next_logits = next_logits / max(1e-6, temperature)\n",
        "\n",
        "        if top_k is not None and top_k > 0:\n",
        "            v, ix = torch.topk(next_logits, k=min(top_k, next_logits.size(-1)))\n",
        "            mask = torch.full_like(next_logits, float(\"-inf\"))\n",
        "            mask.scatter_(1, ix, v)\n",
        "            next_logits = mask\n",
        "\n",
        "        probs = F.softmax(next_logits, dim=-1)    # (1,V)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
        "\n",
        "        tid = int(next_id.item())\n",
        "        generated.append(tid)\n",
        "\n",
        "        last_id = next_id\n",
        "        if tid == vocab.eos_id:\n",
        "            break\n",
        "\n",
        "    return decode_lm(vocab, generated, skip_specials=True)\n"
      ],
      "metadata": {
        "id": "dePdY-Pq_Sbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_inference():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Dataset p√∫blico m√°s grande\n",
        "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "    # Usamos train para vocab; valid para evaluaci√≥n\n",
        "    train_texts_all = [t for t in ds[\"train\"][\"text\"] if t.strip()]\n",
        "    train_texts = train_texts_all[:2500]\n",
        "    valid_texts_all = [t for t in ds[\"validation\"][\"text\"] if t.strip()]\n",
        "    valid_texts = valid_texts_all[:500]\n",
        "\n",
        "    print(\"Train size:\", len(train_texts))\n",
        "    print(\"Valid size:\", len(valid_texts))\n",
        "\n",
        "    # Vocab (whitespace). Puedes subir min_freq o max_vocab\n",
        "    vocab = build_vocab_lm(train_texts, min_freq=2, max_vocab=30000)\n",
        "    print(\"Vocab size:\", len(vocab.itos))\n",
        "\n",
        "    # Convertimos a stream continuo de ids (autorregresivo)\n",
        "    def texts_to_stream(texts: List[str]) -> torch.Tensor:\n",
        "        all_ids = []\n",
        "        for t in texts:\n",
        "            all_ids.extend(encode_lm(vocab, t, add_bos_eos=True))\n",
        "        return torch.tensor(all_ids, dtype=torch.long)\n",
        "\n",
        "    train_ids = texts_to_stream(train_texts)\n",
        "    valid_ids = texts_to_stream(valid_texts)\n",
        "    print(\"Train tokens:\", len(train_ids), \"| Valid tokens:\", len(valid_ids))\n",
        "\n",
        "    # Hiperpar√°metros (mismo protocolo para GRU y LSTM)\n",
        "    block_size = 32\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "    lr = 2e-3\n",
        "    emb_dim = 128\n",
        "    hidden_dim = 256\n",
        "    num_layers = 1\n",
        "    dropout = 0.0\n",
        "    grad_clip = 1.0\n",
        "\n",
        "    train_dl = DataLoader(LMDataset(train_ids, block_size), batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_dl = DataLoader(LMDataset(valid_ids, block_size), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # 1) Entrenar GRU (base)\n",
        "    gru_model = train_one_model(\n",
        "        \"gru\",\n",
        "        train_dl, valid_dl,\n",
        "        vocab_size=len(vocab.itos),\n",
        "        pad_id=vocab.pad_id,\n",
        "        device=device,\n",
        "        emb_dim=emb_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        lr=lr,\n",
        "        epochs=epochs,\n",
        "        grad_clip=grad_clip,\n",
        "        seed=123\n",
        "    )\n",
        "\n",
        "    # Comparaci√≥n final (valid)\n",
        "    gru_nll, gru_ppl = evaluate(gru_model, valid_dl, device)\n",
        "    print(\"\\n=== Comparaci√≥n final (VALID) ===\")\n",
        "    print(f\"GRU : nll={gru_nll:.4f} ppl={gru_ppl:.2f}\")\n",
        "\n",
        "    # Inferencia / generaci√≥n\n",
        "    prompt = \"the meaning of\"\n",
        "    print(\"\\n--- Generaci√≥n (GRU) ---\")\n",
        "    print(generate_next_words(gru_model, vocab, prompt, max_new_tokens=25, temperature=0.9, top_k=50, device=device))\n",
        "\n",
        "    # (Opcional) guardar checkpoints\n",
        "    torch.save({\"model\": gru_model.state_dict(), \"vocab\": vocab}, \"gru_lm.pt\")\n",
        "    print(\"\\nGuardado: gru_lm.pt\")"
      ],
      "metadata": {
        "id": "dcgpZucj_Zg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_inference()"
      ],
      "metadata": {
        "id": "Ht4g9Rm4h-Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurso adicional: Usar RNN para series temporales"
      ],
      "metadata": {
        "id": "E4l33w_6KcDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este bloque trabajaremos un problema de predicci√≥n de series temporales utilizando una red LSTM. El objetivo es entrenar un modelo que, dado un conjunto de valores pasados de una serie, sea capaz de predecir el siguiente valor. Para ello utilizaremos un dataset p√∫blico real: Daily Minimum Temperatures in Melbourne (1981‚Äì1990), que contiene temperaturas m√≠nimas diarias registradas durante 10 a√±os."
      ],
      "metadata": {
        "id": "LHIKHyu4Kn2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -------------------------\n",
        "# 1) Cargar dataset p√∫blico (CSV)\n",
        "# -------------------------\n",
        "# Fuente p√∫blica (GitHub): daily-min-temperatures.csv\n",
        "# (Si prefieres no depender de internet en runtime: descarga el CSV y cambia la ruta a local)\n",
        "CSV_URL = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_URL)\n",
        "# columnas t√≠picas: Date, Temp\n",
        "series = df[\"Temp\"].astype(\"float32\").values\n",
        "print(\"Total puntos:\", len(series))\n",
        "\n",
        "# -------------------------\n",
        "# 2) Split train/valid/test (time-based)\n",
        "# -------------------------\n",
        "n = len(series)\n",
        "train_end = int(n * 0.7)\n",
        "valid_end = int(n * 0.85)\n",
        "\n",
        "train_raw = series[:train_end]\n",
        "valid_raw = series[train_end:valid_end]\n",
        "test_raw  = series[valid_end:]\n",
        "\n",
        "# Normalizaci√≥n (solo con train)\n",
        "mu = train_raw.mean()\n",
        "sigma = train_raw.std() + 1e-8\n",
        "\n",
        "def norm(x): return (x - mu) / sigma\n",
        "def denorm(x): return x * sigma + mu\n",
        "\n",
        "train = norm(train_raw)\n",
        "valid = norm(valid_raw)\n",
        "test  = norm(test_raw)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Dataset de ventanas\n",
        "# -------------------------\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, arr, window=30):\n",
        "        self.arr = torch.tensor(arr, dtype=torch.float32)\n",
        "        self.window = window\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.arr) - self.window)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.arr[idx: idx + self.window]          # (T,)\n",
        "        y = self.arr[idx + self.window]               # ()\n",
        "        return x.unsqueeze(-1), y.unsqueeze(-1)       # (T,1), (1,)\n",
        "\n",
        "window = 30\n",
        "batch_size = 64\n",
        "\n",
        "train_dl = DataLoader(WindowDataset(train, window), batch_size=batch_size, shuffle=True)\n",
        "valid_dl = DataLoader(WindowDataset(valid, window), batch_size=batch_size, shuffle=False)\n",
        "test_dl  = DataLoader(WindowDataset(test, window),  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Modelo LSTM para regresi√≥n 1-step\n",
        "# -------------------------\n",
        "class LSTMForecaster(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        # x: (B,T,1)\n",
        "        out, h = self.lstm(x, h)      # out: (B,T,H)\n",
        "        last = out[:, -1, :]          # (B,H) √∫ltimo paso temporal\n",
        "        yhat = self.fc(last)          # (B,1)\n",
        "        return yhat, h\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = LSTMForecaster(hidden_dim=64, num_layers=1).to(device)\n",
        "\n",
        "# -------------------------\n",
        "# 5) Train/Eval\n",
        "# -------------------------\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_rmse(dl):\n",
        "    model.eval()\n",
        "    se_sum = 0.0\n",
        "    n = 0\n",
        "    for x, y in dl:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        yhat, _ = model(x)\n",
        "        se_sum += ((yhat - y) ** 2).sum().item()\n",
        "        n += y.numel()\n",
        "    rmse = math.sqrt(se_sum / max(1, n))\n",
        "    return rmse\n",
        "\n",
        "epochs = 10\n",
        "grad_clip = 1.0\n",
        "\n",
        "for ep in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    count = 0\n",
        "    for x, y in train_dl:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        yhat, _ = model(x)\n",
        "        loss = loss_fn(yhat, y)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        total += loss.item() * y.numel()\n",
        "        count += y.numel()\n",
        "\n",
        "    train_mse = total / max(1, count)\n",
        "    valid_rmse = eval_rmse(valid_dl)\n",
        "    print(f\"epoch {ep:02d} | train_mse {train_mse:.4f} | valid_rmse {valid_rmse:.4f}\")\n",
        "\n",
        "test_rmse = eval_rmse(test_dl)\n",
        "print(\"\\nTEST RMSE (normalizado):\", test_rmse)\n",
        "\n",
        "# -------------------------\n",
        "# 6) Inferencia: predecir pr√≥ximos k pasos (autoregresivo)\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def forecast_k_steps(model, seed_window, k=7):\n",
        "    \"\"\"\n",
        "    seed_window: array shape (window,) en escala normalizada\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    window_vals = torch.tensor(seed_window, dtype=torch.float32, device=device).view(1, -1, 1)\n",
        "    preds = []\n",
        "    h = None\n",
        "    for _ in range(k):\n",
        "        yhat, h = model(window_vals, h)          # (1,1)\n",
        "        next_val = yhat.item()\n",
        "        preds.append(next_val)\n",
        "        # slide window: quitar primero, a√±adir pred al final\n",
        "        new_seq = torch.cat([window_vals[:, 1:, :], yhat.view(1,1,1)], dim=1)\n",
        "        window_vals = new_seq\n",
        "    return preds\n",
        "\n",
        "# ejemplo con el √∫ltimo window del test\n",
        "seed = test[:window]\n",
        "preds_norm = forecast_k_steps(model, seed, k=7)\n",
        "preds = [denorm(p) for p in preds_norm]\n",
        "print(\"\\nPredicci√≥n pr√≥ximos 7 d√≠as (¬∞C aprox):\", [round(p, 2) for p in preds])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PEGVudWKgev",
        "outputId": "22aae9c6-11af-4cd4-84a7-c0ae0d8db7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total puntos: 3650\n",
            "epoch 01 | train_mse 0.7266 | valid_rmse 0.6722\n",
            "epoch 02 | train_mse 0.4569 | valid_rmse 0.6370\n",
            "epoch 03 | train_mse 0.4411 | valid_rmse 0.6247\n",
            "epoch 04 | train_mse 0.4296 | valid_rmse 0.6148\n",
            "epoch 05 | train_mse 0.4154 | valid_rmse 0.6019\n",
            "epoch 06 | train_mse 0.3967 | valid_rmse 0.5866\n",
            "epoch 07 | train_mse 0.3782 | valid_rmse 0.5901\n",
            "epoch 08 | train_mse 0.3736 | valid_rmse 0.5711\n",
            "epoch 09 | train_mse 0.3757 | valid_rmse 0.5743\n",
            "epoch 10 | train_mse 0.3641 | valid_rmse 0.5693\n",
            "\n",
            "TEST RMSE (normalizado): 0.5493573606485515\n",
            "\n",
            "Predicci√≥n pr√≥ximos 7 d√≠as (¬∞C aprox): [np.float32(7.71), np.float32(7.69), np.float32(7.69), np.float32(7.7), np.float32(7.72), np.float32(7.74), np.float32(7.76)]\n"
          ]
        }
      ]
    }
  ]
}