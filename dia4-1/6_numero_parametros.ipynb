{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, PrefixTuningConfig, TaskType\n",
    "\n",
    "device = \"cuda\"\n",
    "model = \"Qwen/Qwen2.5-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_active_params(model) -> int:\n",
    "    \"\"\"Number of trainable parameters (requires_grad=True).\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_total_params(model) -> int:\n",
    "    \"\"\"Total number of parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def print_active(model, label: str):\n",
    "    active = count_active_params(model)\n",
    "    total = count_total_params(model)\n",
    "    pct = 100.0 * active / total if total > 0 else 0.0\n",
    "    print(f\"{label}: active={active:,}  (of total={total:,}, {pct:.4f}%)\")\n",
    "\n",
    "\n",
    "def load_base_model(model_name: str, trainable: bool, device: str):\n",
    "    \"\"\"\n",
    "    Loads the base model in standard precision.\n",
    "    Note: For 7B, full fine-tuning may not fit on small GPUs.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, device=device)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "\n",
    "    # If not trainable, freeze everything (useful for PEFT base)\n",
    "    if not trainable:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff125112",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_full = load_base_model(model, trainable=True, device=device)\n",
    "print_active(model_full, \"FULL fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_prefix, model_prefix_base = load_base_model(model, trainable=False, device=device)\n",
    "\n",
    "prefix_cfg = PrefixTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=20,   # typical: 10-50\n",
    ")\n",
    "model_prefix = get_peft_model(model_prefix_base, prefix_cfg)\n",
    "print_active(model_prefix, \"PREFIX-tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_lora, model_lora_base = load_base_model(model, trainable=False, device=device)\n",
    "\n",
    "# Typical LoRA targets for LLaMA/Mistral-style architectures:\n",
    "# If your model uses different module names, change target_modules acco rdingly.\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_lora_base, lora_cfg)\n",
    "print_active(model_lora, \"LoRA (r=8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_train_last_k_layers(model, k: int, train_lm_head: bool = True):\n",
    "    \"\"\"\n",
    "    Makes trainable only parameters belonging to the last k transformer layers.\n",
    "    \"\"\"\n",
    "    # Freeze everything first\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Try to find common layer containers\n",
    "    candidates = []\n",
    "    if hasattr(model, \"model\") and hasattr(model.model, \"layers\"):\n",
    "        candidates.append((\"model.model.layers\", model.model.layers))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Model architecture not supported for last-k layer freezing.\")\n",
    "    \n",
    "    if candidates:\n",
    "        _, layers = candidates[0]\n",
    "        n = len(layers)\n",
    "        start = max(0, n - k)\n",
    "        for i in range(start, n):\n",
    "            for p in layers[i].parameters():\n",
    "                p.requires_grad = True\n",
    "    else:\n",
    "        # Fallback: try name-based matching for last-k by scanning layer indices\n",
    "        # This is less robust but often works.\n",
    "        # We detect max layer index in names like \"layers.31.\" or \"h.31.\" etc.\n",
    "        layer_idx = []\n",
    "        for name, _ in model.named_parameters():\n",
    "            m = re.search(r\"(layers|h)\\.(\\d+)\\.\", name)\n",
    "            if m:\n",
    "                layer_idx.append(int(m.group(2)))\n",
    "        if layer_idx:\n",
    "            max_i = max(layer_idx)\n",
    "            start_i = max(0, max_i - (k - 1))\n",
    "            for name, p in model.named_parameters():\n",
    "                m = re.search(r\"(layers|h)\\.(\\d+)\\.\", name)\n",
    "                if m and int(m.group(2)) >= start_i:\n",
    "                    p.requires_grad = True\n",
    "\n",
    "    # Optionally train LM head\n",
    "    if train_lm_head and hasattr(model, \"lm_head\"):\n",
    "        for p in model.lm_head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "LAST_K = 1\n",
    "TRAIN_LM_HEAD = True\n",
    "tokenizer_freeze, model_freeze = load_base_model(model_name=model, trainable=False, device=device)\n",
    "set_train_last_k_layers(model_freeze, LAST_K, TRAIN_LM_HEAD)\n",
    "print_active(model_freeze, f\"Layer freezing (last {LAST_K} layers\" + (\", + lm_head\" if TRAIN_LM_HEAD else \"\") + \")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
