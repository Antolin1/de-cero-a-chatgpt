{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización $L(\\theta)=\\theta^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgAa7FmClFfP",
    "outputId": "49473b38-f9d7-4659-fa45-208942fe0ce3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import display, Math\n",
    "\n",
    "# parámetro inicial\n",
    "theta = torch.tensor(3.0, requires_grad=True)\n",
    "# optimizador, se encarga de actualizar theta\n",
    "optimizer = torch.optim.SGD([theta], lr=0.1)\n",
    "\n",
    "# bucle de optimización (30 pasos)\n",
    "for step in range(30):\n",
    "    # ponemos a cero los gradientes\n",
    "    optimizer.zero_grad()\n",
    "    # función de pérdida theta^2\n",
    "    loss = theta**2\n",
    "    # cálculo de gradientes\n",
    "    loss.backward()\n",
    "    # actualización de parámetros\n",
    "    optimizer.step()\n",
    "    # mostramos el progreso fancy en latex\n",
    "    display(Math(\n",
    "    rf\"\\text{{Paso }} {step + 1}: \\ \\theta \\text{{ se ha movido a }} {theta.item():.3f},\\ \"\n",
    "    rf\"\\text{{loss}} = {loss.item():.4f}\"\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animación del proceso de optimización $L(\\theta)=\\theta^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "BAQ7y0PjlXSB",
    "outputId": "f0f3df65-8386-46e3-fce9-931cca4072d1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# ----- 1. RUN GRADIENT DESCENT AND STORE TRAJECTORY -----\n",
    "\n",
    "theta = torch.tensor(3.0, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([theta], lr=0.1)\n",
    "\n",
    "trajectory = []\n",
    "grads = []\n",
    "\n",
    "for step in range(30):\n",
    "    trajectory.append(theta.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss = theta**2\n",
    "    loss.backward()\n",
    "    grads.append(theta.grad.item())   # store gradient 2θ\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "grads = np.array(grads)\n",
    "\n",
    "\n",
    "# ----- 2. PLOT SETUP -----\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Draw parabola\n",
    "x = np.linspace(-3, 3, 400)\n",
    "ax.plot(x, x**2, label=\"f(θ)=θ²\")\n",
    "\n",
    "# Red point showing θ\n",
    "point, = ax.plot([], [], 'ro', markersize=8)\n",
    "\n",
    "# Gradient arrow\n",
    "arrow = ax.arrow(0, 0, 0, 0, color='blue')\n",
    "\n",
    "# Text info\n",
    "text = ax.text(0.05, 0.9, \"\", transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_xlabel(\"θ\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Optimización de θ² con gradiente descendente\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# ----- 3. ANIMATION FUNCTION -----\n",
    "\n",
    "def update(frame):\n",
    "    θ = trajectory[frame]\n",
    "    grad = grads[frame]\n",
    "\n",
    "    # Update red point\n",
    "    point.set_data([θ], [θ**2])\n",
    "\n",
    "    # Remove old arrow\n",
    "    global arrow\n",
    "    arrow.remove()\n",
    "\n",
    "    # Gradient arrow along x-axis\n",
    "    scale = 1\n",
    "    dx = - scale * grad * 0.1   # gradient points right if grad > 0, left if grad < 0\n",
    "    dy = 0              # no vertical component\n",
    "\n",
    "    arrow = ax.arrow(θ, θ**2, dx, dy, color='blue', width=0.02)\n",
    "\n",
    "    # Update text\n",
    "    text.set_text(f\"step={frame}, θ={θ:.3f}, grad={grad:.3f}, loss={θ**2:.3f}\")\n",
    "\n",
    "    return point, arrow, text\n",
    "\n",
    "\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=len(trajectory), interval=300)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "# ----- 4. DISPLAY IN COLAB -----\n",
    "\n",
    "HTML(anim.to_jshtml())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización $L(\\theta)=\\theta_1^2 + \\theta_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "sPTU8Rh8t0dx",
    "outputId": "2dcc7ae6-58a3-421f-a6ed-149d8e1499e9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# ----- 1. RUN GRADIENT DESCENT AND STORE TRAJECTORY -----\n",
    "\n",
    "theta = torch.tensor([2.5, -1.8], requires_grad=True)  # [theta1, theta2]\n",
    "optimizer = torch.optim.SGD([theta], lr=0.15)\n",
    "\n",
    "trajectory = []\n",
    "grads = []\n",
    "losses = []\n",
    "\n",
    "for step in range(20):\n",
    "    trajectory.append(theta.detach().cpu().numpy().copy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = (theta[0] ** 2) + (theta[1] ** 2)   # theta1^2 + theta2^2\n",
    "    loss.backward()\n",
    "\n",
    "    grads.append(theta.grad.detach().cpu().numpy().copy())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "trajectory = np.array(trajectory)  # shape: (T, 2)\n",
    "grads = np.array(grads)            # shape: (T, 2)\n",
    "losses = np.array(losses)\n",
    "\n",
    "# ----- 2. PLOT SETUP -----\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Contour plot of f(theta1, theta2) = theta1^2 + theta2^2\n",
    "grid = np.linspace(-3, 3, 300)\n",
    "X, Y = np.meshgrid(grid, grid)\n",
    "Z = X**2 + Y**2\n",
    "ax.contour(X, Y, Z, levels=20)\n",
    "\n",
    "# Red point for current theta\n",
    "point, = ax.plot([], [], 'ro', markersize=8)\n",
    "\n",
    "# Gradient arrow (we'll remove & redraw each frame like your code)\n",
    "arrow = ax.arrow(0, 0, 0, 0)\n",
    "\n",
    "# Text info\n",
    "text = ax.text(0.03, 0.97, \"\", transform=ax.transAxes, va=\"top\")\n",
    "\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_xlabel(r\"$\\theta_1$\")\n",
    "ax.set_ylabel(r\"$\\theta_2$\")\n",
    "ax.set_title(r\"Optimización de $f(\\theta_1,\\theta_2)=\\theta_1^2+\\theta_2^2$ con gradiente descendente\")\n",
    "\n",
    "# ----- 3. ANIMATION FUNCTION -----\n",
    "\n",
    "def update(frame):\n",
    "    global arrow\n",
    "\n",
    "    th1, th2 = trajectory[frame]\n",
    "    g1, g2 = grads[frame]\n",
    "    loss = losses[frame]\n",
    "\n",
    "    # Update point\n",
    "    point.set_data([th1], [th2])\n",
    "\n",
    "    # Remove old arrow\n",
    "    arrow.remove()\n",
    "\n",
    "    # Draw arrow in the *descent direction* (-grad) with scaling\n",
    "    lr = 0.15\n",
    "    scale = 1.0\n",
    "    dx = -scale * g1 * lr\n",
    "    dy = -scale * g2 * lr\n",
    "\n",
    "    arrow = ax.arrow(th1, th2, dx, dy, width=0.03, length_includes_head=True)\n",
    "\n",
    "    # Update text (LaTeX for thetas)\n",
    "    text.set_text(\n",
    "        rf\"step={frame}  \"\n",
    "        rf\"$\\theta_1$={th1:.3f}, $\\theta_2$={th2:.3f}  \"\n",
    "        rf\"$\\nabla f$=({g1:.3f},{g2:.3f})  \"\n",
    "        rf\"loss={loss:.3f}\"\n",
    "    )\n",
    "\n",
    "    return point, arrow, text\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=len(trajectory), interval=250)\n",
    "\n",
    "plt.close()\n",
    "HTML(anim.to_jshtml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "\n",
    "Aplica el gradiente descendente para minimizar la función $L(\\theta_1,\\dots,\\theta_{100}) = \\sum_{i=1}^{100} (\\theta_i - 1)^2$. Muestra solo el loss en cada paso y el valor final de $\\theta$ al terminar la optimización.\n",
    "\n",
    "Nota. Debes inicializar $\\theta$ con valores aleatorios. El resultado esperado es que cada $\\theta_i$ tienda a 1 al finalizar la optimización.\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.rand.html\n",
    "- `torch` funciona de manera similar a numpy, por lo que puedes usar operaciones vectorizadas como `theta + 25`, `theta ** 2`, etc.\n",
    "- `.sum()` se usar para sumar todos los elementos de un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW3aG3S3ZNeY"
   },
   "outputs": [],
   "source": [
    "# TODO: Ejercicio"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
