{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdIn_TLgCAMP"
   },
   "source": [
    "# Arquitectura Transformer\n",
    "Este notebook está diseñado para acompañar la sección **Arquitectura Transformer** de las diapositivas.\n",
    "\n",
    "Incluye:\n",
    "- Repaso rápido de PLN (tareas típicas)\n",
    "- Atención (Q,K,V) con intuición + matemática\n",
    "- Self-attention, máscara causal, multi-head attention\n",
    "- Residual + LayerNorm (Pre-Norm) + FFN\n",
    "- Encoder, Decoder, y Transformer Encoder–Decoder **desde cero** (mini-implementación sencilla)\n",
    "- Ejercicios (TODO) intercalados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "FQl_v8ZM5Zn9",
    "outputId": "ac30c91e-0cc1-4985-ba2a-6431d7ec4057"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lc2FclQhCMkr",
    "outputId": "64ddca46-0a89-4aa2-ec65-908926dad6d7"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "print('Seed set with value ', seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNe9QUjECU-7"
   },
   "source": [
    "## 1) Mini repaso PLN (muy breve)\n",
    "Tareas comunes:\n",
    "- Tokenización / subwords\n",
    "- POS / NER\n",
    "- Clasificación (sentimiento, spam)\n",
    "- Traducción / resumen\n",
    "- QA\n",
    "\n",
    "Problema central: **texto → números** (IDs → embeddings).\n",
    "\n",
    "## 2) Atención: intuición y fórmula\n",
    "### Intuición (Q, K, V)\n",
    "- **Query (Q)**: lo que *busco*.\n",
    "- **Key (K)**: cómo describo cada elemento *disponible*.\n",
    "- **Value (V)**: la información que *me llevo* si atiendo a ese elemento.\n",
    "\n",
    "La atención hace:\n",
    "1) calcula similitud entre Q y K (scores)\n",
    "2) normaliza con softmax → pesos\n",
    "3) combina V con esos pesos → salida\n",
    "\n",
    "### Fórmula\n",
    "$$\\mathrm{Att}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf7xmz1ICaq9"
   },
   "source": [
    "### Implementación base: scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwlHl-qJCW5D",
    "outputId": "6134a9fd-bcea-4860-be85-b52b999809b1"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None, dropout_p=0.0):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention (núcleo del Transformer).\n",
    "\n",
    "    Q: (B, h, Tq, d)  queries\n",
    "    K: (B, h, Tk, d)  keys\n",
    "    V: (B, h, Tk, d)  values\n",
    "    mask: (1 o B, 1 o h, Tq, Tk)\n",
    "          0    -> permitido\n",
    "          -inf -> bloqueado\n",
    "    \"\"\"\n",
    "    # Dimensión por head\n",
    "    d = Q.size(-1)\n",
    "\n",
    "    # Producto punto QK^T escalado\n",
    "    # Resultado: (B, h, Tq, Tk)\n",
    "    scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d)\n",
    "\n",
    "    # Aplicamos máscara si existe (padding o causal)\n",
    "    if mask is not None:\n",
    "        scores = scores + mask\n",
    "\n",
    "    # Softmax sobre las keys (última dimensión)\n",
    "    attn = F.softmax(scores.float(), dim=-1).type_as(scores)\n",
    "\n",
    "    # Dropout opcional sobre la atención\n",
    "    if dropout_p > 0:\n",
    "        attn = F.dropout(attn, p=dropout_p, training=True)\n",
    "\n",
    "    # Combinación ponderada de los values\n",
    "    # Resultado: (B, h, Tq, d)\n",
    "    out = attn @ V\n",
    "\n",
    "    return out, attn\n",
    "\n",
    "# Test rápido de shapes\n",
    "B,h,T,d = 2, 4, 5, 8\n",
    "Q = torch.randn(B,h,T,d)\n",
    "K = torch.randn(B,h,T,d)\n",
    "V = torch.randn(B,h,T,d)\n",
    "out, attn = scaled_dot_product_attention(Q,K,V)\n",
    "print('out:', out.shape)\n",
    "print('attn:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYRniC3WDJI6"
   },
   "source": [
    "## Ejercicio 1 (shapes y softmax)\n",
    "**TODO:**\n",
    "1) Comprueba que `attn.sum(dim=-1)` vale 1 (aprox) en cada query.\n",
    "2) Explica por qué el softmax se aplica en la dimensión de `Tk` (tokens a los que atiendes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnR9LESGDHY1",
    "outputId": "c3a9296a-03ab-4159-b432-84b7bfd67ba1"
   },
   "outputs": [],
   "source": [
    "# Solución sugerida\n",
    "sums = attn.sum(dim=-1)\n",
    "print('attn sum shape:', sums.shape)\n",
    "print('min/max:', sums.min().item(), sums.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPrFA19wDMeY"
   },
   "source": [
    "## 3) Máscara causal (masked self-attention)\n",
    "En modelos autoregresivos (tipo GPT), el token en posición *t* **no puede ver el futuro**.\n",
    "\n",
    "Creamos una máscara triangular superior con `-inf` para bloquear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1B_VTS1DO30",
    "outputId": "8585ac78-9685-40ff-87e3-be51da0f8867"
   },
   "outputs": [],
   "source": [
    "def causal_mask(T, device=None):\n",
    "    # (1,1,T,T)\n",
    "    m = torch.full((1,1,T,T), float('-inf'), device=device)\n",
    "    m = torch.triu(m, diagonal=1)\n",
    "    return m\n",
    "\n",
    "T=6\n",
    "m = causal_mask(T)\n",
    "print(m[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIeV2e2Dj9F"
   },
   "source": [
    "### Demo: la máscara hace que la atención al futuro sea 0 tras softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhvN7MS4DjAU",
    "outputId": "900d3383-63aa-4326-d191-6852e94726bb"
   },
   "outputs": [],
   "source": [
    "B,h,T,d = 1,1,5,4\n",
    "Q = torch.randn(B,h,T,d)\n",
    "K = torch.randn(B,h,T,d)\n",
    "V = torch.randn(B,h,T,d)\n",
    "mask = causal_mask(T)\n",
    "_, attn_masked = scaled_dot_product_attention(Q,K,V,mask=mask)\n",
    "print('attn (masked) matrix:')\n",
    "print(attn_masked[0,0].detach())\n",
    "print('Suma por fila:', attn_masked[0,0].sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDY7em4mDm2l"
   },
   "source": [
    "## Ejercicio 2 (interpretación de máscara)\n",
    "**TODO:**\n",
    "1) ¿Qué posiciones exactas se bloquean cuando `diagonal=1`?\n",
    "2) ¿Por qué dejamos la diagonal en 0 (permitida)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2iCOlk44Dpv7",
    "outputId": "54fb3957-a839-410a-81de-bfb968fafd88"
   },
   "outputs": [],
   "source": [
    "def causal_mask(T, device=None):\n",
    "    # (1,1,T,T)\n",
    "    m = torch.full((1,1,T,T), float('-inf'), device=device)\n",
    "    m = torch.triu(m, diagonal=1)\n",
    "    return m\n",
    "\n",
    "T=6\n",
    "m = causal_mask(T)\n",
    "print(m[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooy7SagyD6H5"
   },
   "source": [
    "se bloquean exactamente todas las posiciones por encima de la diagonal principal.\n",
    "\n",
    "Formalmente, para una matriz (T, T):\n",
    "\n",
    "*   Se bloquean las posiciones (i, j) tales que\n",
    "j > i\n",
    "*   Se permiten las posiciones (i, j) tales que\n",
    "j ≤ i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27C8OjXlEIpk"
   },
   "source": [
    "La diagonal corresponde a i == j, es decir: **el token se atiende a sí mismo**\n",
    "\n",
    "¿Por qué esto es necesario?\n",
    "\n",
    "1. Identidad / copia\n",
    "* El token actual debe poder usar su propia representación.\n",
    "* Si bloqueas la diagonal, el token no puede verse a sí mismo → pierdes información.\n",
    "\n",
    "2. Estabilidad del modelo\n",
    "\n",
    "* En la práctica, mucha información útil viene de la propia posición.\n",
    "\n",
    "* Bloquear la diagonal hace el entrenamiento inestable y peor.\n",
    "\n",
    "3. Interpretación correcta del LM\n",
    "* En un modelo autoregresivo, queremos: \"puedes usar el pasado y el presente, pero no el futuro\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUcmL7ryEjyD"
   },
   "source": [
    "## 4) Multi-Head Attention (MHA)\n",
    "En vez de una sola atención, usamos **h cabezas**. Cada cabeza trabaja en un subespacio de dimensión `head_dim = d_model / h`.\n",
    "\n",
    "Pasos:\n",
    "1) Proyectar: Q=XWq, K=XWk, V=XWv\n",
    "2) Separar en cabezas (reshape)\n",
    "3) Atención por cabeza\n",
    "4) Concatenar cabezas\n",
    "5) Proyección final Wo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4ykn6WJEhfp",
    "outputId": "1e320a3a-bdf1-4e87-b3d0-a950abeb3e5a"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación de Multi-Head Attention (self- o cross-attention).\n",
    "\n",
    "    - d_model: dimensión total del modelo\n",
    "    - n_heads: número de cabezas de atención\n",
    "    - is_causal: si True, usa máscara causal (autoregresiva)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.0, is_causal=False, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        # d_model debe poder dividirse entre el número de heads\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads  # dimensión por cabeza\n",
    "        self.is_causal = is_causal\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Proyecciones lineales para Queries, Keys y Values\n",
    "        # Cada una proyecta d_model -> d_model\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Proyección final para recombinar las heads\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Si es atención causal, precomputamos la máscara triangular superior\n",
    "        # Shape: (1, 1, max_seq_len, max_seq_len)\n",
    "        if is_causal:\n",
    "            m = torch.full((1, 1, max_seq_len, max_seq_len), float('-inf'))\n",
    "            m = torch.triu(m, diagonal=1)\n",
    "            # register_buffer => se mueve con .to(device), pero no es un parámetro entrenable\n",
    "            self.register_buffer('mask', m)\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def forward(self, x, kv=None):\n",
    "        \"\"\"\n",
    "        x:  (B, T, d_model)\n",
    "        kv: (B, Tk, d_model) o None\n",
    "\n",
    "        - Si kv=None: self-attention (Q,K,V vienen de x)\n",
    "        - Si kv!=None: cross-attention (Q de x, K/V de kv)\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Self-attention si no se proporciona kv\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "\n",
    "        B2, Tk, _ = kv.shape\n",
    "        assert B2 == B  # batch debe coincidir\n",
    "\n",
    "        # Proyecciones lineales\n",
    "        # q: (B,T,d)   k,v: (B,Tk,d)\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(kv)\n",
    "        v = self.wv(kv)\n",
    "\n",
    "        # Reorganizamos para multi-head:\n",
    "        # (B,T,d) -> (B,h,T,head_dim)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, Tk, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, Tk, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Seleccionamos la porción necesaria de la máscara causal\n",
    "        mask = None\n",
    "        if self.is_causal:\n",
    "            # (1,1,T,Tk) – se broadcasta a (B,h,T,Tk)\n",
    "            mask = self.mask[:, :, :T, :Tk]\n",
    "\n",
    "        # Atención escalada\n",
    "        # out:  (B,h,T,head_dim)\n",
    "        # attn: (B,h,T,Tk)\n",
    "        out, attn = scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            mask=mask,\n",
    "            dropout_p=self.dropout\n",
    "        )\n",
    "\n",
    "        # Recombinar las heads:\n",
    "        # (B,h,T,head_dim) -> (B,T,d_model)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "\n",
    "        # Proyección final\n",
    "        out = self.wo(out)\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Test de shapes\n",
    "# -------------------------\n",
    "mha = MultiHeadAttention(\n",
    "    d_model=32,\n",
    "    n_heads=4,\n",
    "    is_causal=True,\n",
    "    max_seq_len=64\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 10, 32)\n",
    "\n",
    "y, attn = mha(x)\n",
    "\n",
    "print('y:', y.shape)       # (B,T,d_model) = (2,10,32)\n",
    "print('attn:', attn.shape) # (B,h,T,T) = (2,4,10,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0dOhcsQFbjw"
   },
   "source": [
    "## 5) Bloques del Transformer: FFN, LayerNorm, Residual (Pre-Norm)\n",
    "Un bloque típico (Pre-Norm) hace:\n",
    "- `x = x + Attention(LN(x))`\n",
    "- `x = x + FFN(LN(x))`\n",
    "\n",
    "El FFN es *position-wise*: la misma MLP se aplica a cada posición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_XHGuCUFX9Z",
    "outputId": "d61e083e-9133-4947-f160-9d32192ace93"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-Forward Network del Transformer.\n",
    "    Se aplica de forma independiente en cada posición.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, hidden_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Proyección d_model -> hidden_dim\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim)\n",
    "\n",
    "        # Proyección hidden_dim -> d_model\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model)\n",
    "\n",
    "        # Dropout para regularización\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, d_model)\n",
    "        \"\"\"\n",
    "        return self.drop(\n",
    "            self.w2(\n",
    "                F.relu(self.w1(x))\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa Encoder Transformer (Pre-LayerNorm).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, ffn_hidden, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalización antes de la atención\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            is_causal=False\n",
    "        )\n",
    "\n",
    "        # Normalización antes de la FFN\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, d_model)\n",
    "        devuelve: (B, T, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # ---- Bloque de self-attention ----\n",
    "        # Normalizamos primero (Pre-LN)\n",
    "        h, _ = self.attn(self.ln1(x))\n",
    "\n",
    "        # Residual connection\n",
    "        x = x + h\n",
    "\n",
    "        # ---- Bloque Feed-Forward ----\n",
    "        # Normalizamos antes de la FFN\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "layer = TransformerEncoderLayer(d_model=32, n_heads=4, ffn_hidden=64, dropout=0.1)\n",
    "x = torch.randn(2, 7, 32)\n",
    "y = layer(x)\n",
    "print('y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFOm7pAIEzNo"
   },
   "source": [
    "## 6) Positional Encoding (sinusoidal)\n",
    "La atención no sabe orden por sí sola. Sumamos una codificación posicional.\n",
    "Implementamos la sinusoidal del paper clásico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZ4ce3WNEm57",
    "outputId": "e00eb3c6-0215-4d3d-eca6-e72b3a02ecd5"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding sinusoidal (Vaswani et al., 2017).\n",
    "\n",
    "    Añade información de posición a los embeddings de entrada.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Tabla de codificaciones posicionales\n",
    "        # Shape: (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Posiciones: (max_len, 1)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        # Términos de escala para las frecuencias\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Dimensiones pares → seno\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Dimensiones impares → coseno\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Guardamos como buffer (no entrenable)\n",
    "        # Shape final: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, d_model)\n",
    "        devuelve: (B, T, d_model)\n",
    "        \"\"\"\n",
    "        # Sumamos la codificación posicional hasta la longitud T\n",
    "        return x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "\n",
    "pe = PositionalEncoding(max_len=50, d_model=32)\n",
    "x = torch.zeros(1, 10, 32)\n",
    "y = pe(x)\n",
    "print('PE added shape:', y.shape)\n",
    "print('Primer vector pos0 (primeras 6 dims):', y[0,0,:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBp1RsZME96f"
   },
   "source": [
    "## 7) Encoder (stack de capas)\n",
    "El encoder aplica N veces el bloque self-attention + ffn.\n",
    "Salida: representaciones contextualizadas para cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TkvFVRdE_ok",
    "outputId": "bd7f209a-5a67-4d92-987b-fc91c08c0b98"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Transformer: pila de capas TransformerEncoderLayer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, ffn_hidden, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Creamos una lista de capas encoder idénticas en estructura\n",
    "        # (pero con parámetros independientes)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                ffn_hidden=ffn_hidden,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Normalización final\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, d_model)\n",
    "        devuelve: (B, T, d_model)\n",
    "        \"\"\"\n",
    "        # Pasamos la entrada por cada capa del encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Normalización final\n",
    "        return self.ln(x)\n",
    "\n",
    "enc = Encoder(n_layers=2, d_model=32, n_heads=4, ffn_hidden=64, dropout=0.1)\n",
    "x = torch.randn(2, 8, 32)\n",
    "h = enc(x)\n",
    "print('enc out:', h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDHjDIfJFm_8"
   },
   "source": [
    "## 8) Decoder layer (masked self-attn + cross-attn + ffn)\n",
    "El decoder en un Transformer de traducción tiene 3 subcapas:\n",
    "1) Masked self-attention (causal)\n",
    "2) Cross-attention (Q del decoder, K/V del encoder)\n",
    "3) FFN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gSz-b2qFjQI",
    "outputId": "1673ff4d-79e5-4bfb-8a4b-4aaef053dff8"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa Decoder Transformer (estilo Pre-LayerNorm) con:\n",
    "      1) masked self-attention (causal)\n",
    "      2) cross-attention (atiende a la salida del encoder)\n",
    "      3) feed-forward network (FFN)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, ffn_hidden, dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---- Bloque 1: masked self-attention ----\n",
    "        # LN antes del sub-bloque (Pre-LN)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        # Self-attention causal: cada posición t solo puede mirar a <= t\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            d_model, n_heads, dropout=dropout, is_causal=True, max_seq_len=max_seq_len\n",
    "        )\n",
    "\n",
    "        # ---- Bloque 2: cross-attention ----\n",
    "        # LN antes del sub-bloque (Pre-LN)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        # Cross-attention NO causal: Q viene del decoder, K/V vienen del encoder\n",
    "        self.cross_attn = MultiHeadAttention(\n",
    "            d_model, n_heads, dropout=dropout, is_causal=False\n",
    "        )\n",
    "\n",
    "        # ---- Bloque 3: FFN ----\n",
    "        # LN antes del sub-bloque (Pre-LN)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        \"\"\"\n",
    "        x:       (B, T_dec, d_model)  estados/embeddings del decoder (entrada parcial)\n",
    "        enc_out: (B, T_enc, d_model)  salida del encoder (memoria)\n",
    "        devuelve:\n",
    "        x:       (B, T_dec, d_model)  salida del decoder layer\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Masked self-attention (causal)\n",
    "        # Normalizamos x -> aplicamos self-attn -> residual\n",
    "        h, _ = self.self_attn(self.ln1(x))  # h: (B, T_dec, d_model)\n",
    "        x = x + h\n",
    "\n",
    "        # 2) Cross-attention (Q = x, K/V = enc_out)\n",
    "        # Normalizamos x -> aplicamos cross-attn con kv=enc_out -> residual\n",
    "        h, _ = self.cross_attn(self.ln2(x), kv=enc_out)  # h: (B, T_dec, d_model)\n",
    "        x = x + h\n",
    "\n",
    "        # 3) Feed-Forward Network\n",
    "        # Normalizamos -> FFN -> residual\n",
    "        x = x + self.ffn(self.ln3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Test de shapes\n",
    "# -------------------------\n",
    "dec_layer = TransformerDecoderLayer(\n",
    "    d_model=32, n_heads=4, ffn_hidden=64, dropout=0.1, max_seq_len=64\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 6, 32)       # (B=2, T_dec=6, d_model=32)\n",
    "enc_out = torch.randn(2, 8, 32) # (B=2, T_enc=8, d_model=32)\n",
    "\n",
    "y = dec_layer(x, enc_out)\n",
    "\n",
    "print('dec layer out:', y.shape)  # esperado: (2, 6, 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ovC9wMoF5mk"
   },
   "source": [
    "## 9) Decoder (stack)\n",
    "Apilamos N capas de decoder y normalizamos al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgZHymMeF7eZ",
    "outputId": "387a3fa9-62e1-4b34-dffa-8caab32aa194"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Transformer: pila de capas TransformerDecoderLayer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, ffn_hidden,\n",
    "                 dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        # Pila de capas decoder (cada una con parámetros independientes)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                ffn_hidden=ffn_hidden,\n",
    "                dropout=dropout,\n",
    "                max_seq_len=max_seq_len\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Normalización final\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        \"\"\"\n",
    "        x:       (B, T_dec, d_model)  tokens/estados del decoder\n",
    "        enc_out: (B, T_enc, d_model)  salida del encoder (memoria)\n",
    "        devuelve:\n",
    "        out:     (B, T_dec, d_model)\n",
    "        \"\"\"\n",
    "        # Pasamos por todas las capas del decoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out)\n",
    "\n",
    "        # Normalización final\n",
    "        return self.ln(x)\n",
    "\n",
    "dec = Decoder(n_layers=2, d_model=32, n_heads=4, ffn_hidden=64, dropout=0.1, max_seq_len=64)\n",
    "x = torch.randn(2, 6, 32)\n",
    "enc_out = torch.randn(2, 8, 32)\n",
    "y = dec(x, enc_out)\n",
    "print('decoder out:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVHQcmhRGMDH"
   },
   "source": [
    "## 10) Transformer completo (Encoder–Decoder) mini\n",
    "Incluimos:\n",
    "- Embedding de tokens\n",
    "- Positional encoding\n",
    "- Encoder\n",
    "- Decoder\n",
    "- Proyección final a vocab (lm_head)\n",
    "\n",
    "Para que sea autocontenido, hacemos un vocab mini y datos pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1zg7d4CGNtY",
    "outputId": "adbe4b68-ea7b-4fe1-9e35-9e362f62cc2f"
   },
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini Transformer Encoder-Decoder (seq2seq) para modelado de tokens.\n",
    "\n",
    "    Flujo:\n",
    "      src_idx (B,S) -> tok_emb + pos_enc -> Encoder -> enc_out (B,S,d)\n",
    "      tgt_idx (B,T) -> tok_emb + pos_enc -> Decoder (con cross-attn a enc_out) -> dec_out (B,T,d)\n",
    "      dec_out -> lm_head -> logits (B,T,vocab)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, max_len, d_model=64, n_heads=4,\n",
    "                 n_layers=2, ffn_hidden=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Embedding de tokens: ids -> vectores (d_model)\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding sinusoidal (no entrenable)\n",
    "        self.pos_enc = PositionalEncoding(max_len, d_model)\n",
    "\n",
    "        # Dropout aplicado a las representaciones de entrada (emb + pos)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder: pila de TransformerEncoderLayer (self-attn no causal)\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, ffn_hidden, dropout)\n",
    "\n",
    "        # Decoder: pila de TransformerDecoderLayer:\n",
    "        # - self-attn causal (autoregresivo)\n",
    "        # - cross-attn hacia enc_out\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, ffn_hidden, dropout, max_seq_len=max_len)\n",
    "\n",
    "        # Cabeza de salida a vocabulario: (d_model -> vocab_size)\n",
    "        # bias=False es común en LM heads (y a veces se comparte con embeddings)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, src_idx, tgt_idx, tgt_targets=None):\n",
    "        \"\"\"\n",
    "        src_idx:     (B, S) ids fuente (entrada al encoder)\n",
    "        tgt_idx:     (B, T) ids del decoder (tokens vistos hasta ahora: teacher forcing)\n",
    "        tgt_targets: (B, T) ids objetivo (lo que queremos predecir en cada paso)\n",
    "                     Si se pasa, devolvemos también la loss.\n",
    "\n",
    "        Devuelve:\n",
    "          logits: (B, T, vocab_size)\n",
    "          loss:   escalar o None\n",
    "        \"\"\"\n",
    "        # Comprobaciones de tamaño\n",
    "        B, S = src_idx.shape\n",
    "        B2, T = tgt_idx.shape\n",
    "        assert B == B2\n",
    "        assert S <= self.max_len and T <= self.max_len\n",
    "\n",
    "        # -------------------------\n",
    "        # Encoder\n",
    "        # -------------------------\n",
    "        # 1) Token embedding: (B,S) -> (B,S,d)\n",
    "        # 2) Sumar positional encoding: (B,S,d)\n",
    "        # 3) Dropout\n",
    "        src = self.drop(self.pos_enc(self.tok_emb(src_idx)))   # (B,S,d_model)\n",
    "\n",
    "        # Encoder produce una representación contextual por posición\n",
    "        enc_out = self.encoder(src)                            # (B,S,d_model)\n",
    "\n",
    "        # -------------------------\n",
    "        # Decoder\n",
    "        # -------------------------\n",
    "        # Igual para el decoder:\n",
    "        # embeddings + pos enc + dropout\n",
    "        tgt = self.drop(self.pos_enc(self.tok_emb(tgt_idx)))   # (B,T,d_model)\n",
    "\n",
    "        # Decoder usa:\n",
    "        # - self-attn causal dentro del tgt (no mirar al futuro)\n",
    "        # - cross-attn con enc_out (mirar al src)\n",
    "        dec_out = self.decoder(tgt, enc_out)                   # (B,T,d_model)\n",
    "\n",
    "        # -------------------------\n",
    "        # Proyección a vocabulario\n",
    "        # -------------------------\n",
    "        logits = self.lm_head(dec_out)                         # (B,T,vocab_size)\n",
    "\n",
    "        # -------------------------\n",
    "        # Loss (opcional)\n",
    "        # -------------------------\n",
    "        loss = None\n",
    "        if tgt_targets is not None:\n",
    "            # CrossEntropyLoss espera:\n",
    "            # logits: (N, C) y targets: (N,)\n",
    "            # Por eso aplanamos (B,T, vocab) -> (B*T, vocab) y (B,T) -> (B*T)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                tgt_targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Mini prueba de shapes\n",
    "# -------------------------\n",
    "model = MiniTransformer(vocab_size=50, max_len=32, d_model=64, n_heads=4, n_layers=2).to(device)\n",
    "\n",
    "# src: (B=2, S=10)\n",
    "src = torch.randint(0, 50, (2, 10)).to(device)\n",
    "\n",
    "# tgt: (B=2, T=8)\n",
    "tgt = torch.randint(0, 50, (2, 8)).to(device)\n",
    "\n",
    "logits, _ = model(src, tgt)\n",
    "\n",
    "print('logits:', logits.shape)  # esperado: (2, 8, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXR6G2CRGhKg"
   },
   "source": [
    "### Ejemplo de uso: Traducción de números\n",
    "\n",
    "Construimos una tarea artificial para entrenar rápido:\n",
    "- Entrada: secuencia de dígitos (0-9)\n",
    "- Salida: secuencia \"traducida\" (dígito + 10) (solo para tener un mapeo)\n",
    "\n",
    "Esto nos permite practicar el pipeline encoder-decoder sin datasets externos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AaVHlOTGnLk",
    "outputId": "f8ee44b3-1fd2-4b45-99ef-25d111cc3c9f"
   },
   "outputs": [],
   "source": [
    "PAD, BOS, EOS = 0, 1, 2\n",
    "\n",
    "def make_toy_batch(batch_size=32, min_len=3, max_len=8, vocab_size=32):\n",
    "    \"\"\"\n",
    "    Crea un batch sintético para un Transformer encoder-decoder.\n",
    "\n",
    "    La tarea es \"traducir\" una secuencia de dígitos a otro vocabulario.\n",
    "    \"\"\"\n",
    "    src_seqs = []      # entradas del encoder\n",
    "    tgt_in_seqs = []   # entradas del decoder (con BOS)\n",
    "    tgt_out_seqs = []  # objetivos del decoder (con EOS)\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        # Longitud aleatoria de la secuencia\n",
    "        L = random.randint(min_len, max_len)\n",
    "\n",
    "        # Dígitos aleatorios (0..9)\n",
    "        digits = [random.randint(0, 9) for _ in range(L)]\n",
    "\n",
    "        # Encoder input:\n",
    "        # dígitos codificados en 3..12 + EOS\n",
    "        src = [3 + d for d in digits] + [EOS]\n",
    "\n",
    "        # Decoder target:\n",
    "        # dígitos \"traducidos\" en 13..22 + EOS\n",
    "        tgt_out = [13 + d for d in digits] + [EOS]\n",
    "\n",
    "        # Decoder input (teacher forcing):\n",
    "        # BOS + tokens traducidos (sin EOS)\n",
    "        tgt_in = [BOS] + [13 + d for d in digits]\n",
    "\n",
    "        src_seqs.append(src)\n",
    "        tgt_in_seqs.append(tgt_in)\n",
    "        tgt_out_seqs.append(tgt_out)\n",
    "\n",
    "    # Función de padding a la longitud máxima del batch\n",
    "    def pad(seqs, pad_id=PAD):\n",
    "        T = max(len(s) for s in seqs)\n",
    "        out = torch.full((len(seqs), T), pad_id, dtype=torch.long)\n",
    "        for i, s in enumerate(seqs):\n",
    "            out[i, :len(s)] = torch.tensor(s)\n",
    "        return out\n",
    "\n",
    "    # Devolvemos tensores (B, T)\n",
    "    return pad(src_seqs), pad(tgt_in_seqs), pad(tgt_out_seqs)\n",
    "\n",
    "\n",
    "# Ejemplo de batch pequeño\n",
    "src, tgt_in, tgt_out = make_toy_batch(batch_size=4)\n",
    "\n",
    "print('src:\\n', src)\n",
    "print('tgt_in:\\n', tgt_in)\n",
    "print('tgt_out:\\n', tgt_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezBmxd8XG2Ul"
   },
   "source": [
    "## 12) Entrenamiento rápido\n",
    "Objetivo: minimizar cross-entropy del decoder para predecir `tgt_out` dado `tgt_in` y `enc_out`.\n",
    "Esto es teacher forcing en traducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "Hch41297G0HZ",
    "outputId": "5735acf9-7124-4a79-e799-e6b30c04f4d4"
   },
   "outputs": [],
   "source": [
    "toy_vocab_size = 32\n",
    "\n",
    "# Inicializamos el modelo Transformer encoder-decoder\n",
    "model = MiniTransformer(\n",
    "    vocab_size=toy_vocab_size,\n",
    "    max_len=32,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    ffn_hidden=128,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Optimizador Adam (típico en Transformers)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Para guardar la evolución de la loss\n",
    "loss_hist = []\n",
    "\n",
    "# Entrenamiento\n",
    "for step in range(200):\n",
    "    model.train()\n",
    "\n",
    "    # Batch sintético (teacher forcing)\n",
    "    src, tgt_in, tgt_out = make_toy_batch(batch_size=64)\n",
    "    src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "\n",
    "    # Reset de gradientes\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Forward + loss\n",
    "    _, loss = model(src, tgt_in, tgt_targets=tgt_out)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Paso del optimizador\n",
    "    opt.step()\n",
    "\n",
    "    # Guardamos loss\n",
    "    loss_hist.append(loss.item())\n",
    "\n",
    "    # Logging cada 50 pasos\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f'step {step+1:03d} | loss {loss.item():.4f}')\n",
    "\n",
    "# Gráfica de la loss\n",
    "plt.figure()\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('seq2seq training loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kvT9cknHEUr"
   },
   "source": [
    "## 13) Inferencia greedy (generación en decoder)\n",
    "Generamos token a token con el decoder, usando máscara causal.\n",
    "Usamos greedy (argmax) para simplificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaSF9zxQHNa8",
    "outputId": "8aa94863-c8ea-4079-c74a-3cc52a9a129d"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(model, src_idx, max_new_tokens=20):\n",
    "    \"\"\"\n",
    "    Decodificación autoregresiva greedy para un Transformer encoder-decoder.\n",
    "\n",
    "    model: modelo entrenado\n",
    "    src_idx: (B, S) ids de entrada\n",
    "    max_new_tokens: límite de generación\n",
    "    devuelve:\n",
    "      ys: (B, T_gen) secuencia generada (incluye BOS y EOS)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src_idx = src_idx.to(device)\n",
    "\n",
    "    # -------------------------\n",
    "    # Encoder (una sola vez)\n",
    "    # -------------------------\n",
    "    src = model.drop(model.pos_enc(model.tok_emb(src_idx)))\n",
    "    enc_out = model.encoder(src)   # (B, S, d_model)\n",
    "\n",
    "    # -------------------------\n",
    "    # Inicializamos el decoder con BOS\n",
    "    # -------------------------\n",
    "    B = src_idx.size(0)\n",
    "    ys = torch.full((B, 1), BOS, dtype=torch.long, device=device)\n",
    "\n",
    "    # -------------------------\n",
    "    # Decodificación autoregresiva\n",
    "    # -------------------------\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Embedding + pos enc del decoder\n",
    "        tgt = model.drop(model.pos_enc(model.tok_emb(ys)))\n",
    "\n",
    "        # Decoder con cross-attention al encoder\n",
    "        dec_out = model.decoder(tgt, enc_out)\n",
    "\n",
    "        # Logits del último timestep\n",
    "        logits = model.lm_head(dec_out)[:, -1, :]  # (B, vocab)\n",
    "\n",
    "        # Elección greedy del siguiente token\n",
    "        next_tok = logits.argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "\n",
    "        # Añadimos el token generado\n",
    "        ys = torch.cat([ys, next_tok], dim=1)\n",
    "\n",
    "        # Si todas las secuencias generaron EOS, paramos\n",
    "        if (next_tok == EOS).all():\n",
    "            break\n",
    "\n",
    "    return ys\n",
    "\n",
    "# Probamos\n",
    "src, tgt_in, tgt_out = make_toy_batch(batch_size=3)\n",
    "pred = greedy_decode(model, src)\n",
    "print('src:', src)\n",
    "print('target_out:', tgt_out)\n",
    "print('pred:', pred.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoamuNc2HvgX"
   },
   "source": [
    "### **Ejercicio: hacer una tarea sencilla de traducción de inglés a español**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXtv2jDlIc10"
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Vocab + encode/decode\n",
    "# -------------------------\n",
    "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
    "\n",
    "@dataclass\n",
    "class Vocab:\n",
    "    stoi: Dict[str, int]\n",
    "    itos: List[str]\n",
    "    pad: str = \"<pad>\"\n",
    "    bos: str = \"<bos>\"\n",
    "    eos: str = \"<eos>\"\n",
    "    unk: str = \"<unk>\"\n",
    "\n",
    "    @property\n",
    "    def pad_id(self): return self.stoi[self.pad]\n",
    "    @property\n",
    "    def bos_id(self): return self.stoi[self.bos]\n",
    "    @property\n",
    "    def eos_id(self): return self.stoi[self.eos]\n",
    "    @property\n",
    "    def unk_id(self): return self.stoi[self.unk]\n",
    "\n",
    "def build_vocab_from_parallel(pairs: List[Tuple[str, str]], min_freq: int = 1) -> Vocab:\n",
    "    # Vocab compartido (porque MiniTransformer tiene un solo embedding tok_emb)\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for en, es in pairs:\n",
    "        c.update(en.lower().split())\n",
    "        c.update(es.lower().split())\n",
    "    specials = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]  # ids 0..3\n",
    "    itos = specials + [w for w, f in c.items() if f >= min_freq and w not in specials]\n",
    "    stoi = {w: i for i, w in enumerate(itos)}\n",
    "    return Vocab(stoi=stoi, itos=itos)\n",
    "\n",
    "def encode_text(v: Vocab, text: str) -> List[int]:\n",
    "    toks = text.lower().split()\n",
    "    return [v.stoi.get(w, v.unk_id) for w in toks]\n",
    "\n",
    "def decode_ids(v: Vocab, ids: List[int]) -> str:\n",
    "    return \" \".join(v.itos[i] if 0 <= i < len(v.itos) else \"<oov>\" for i in ids)\n",
    "\n",
    "def pad_2d(seqs: List[List[int]], pad_id: int) -> torch.Tensor:\n",
    "    T = max(len(s) for s in seqs)\n",
    "    out = torch.full((len(seqs), T), pad_id, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xg8WMKfaIiwr",
    "outputId": "53cfe4f5-8af5-4902-d550-3febac8bc5ee"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2) Datos EN->ES (muy pequeños, para demo)\n",
    "#    (Puedes añadir más pares para mejorar)\n",
    "# -------------------------\n",
    "pairs = [\n",
    "    (\"i like pizza\",            \"me gusta la pizza\"),\n",
    "    (\"i like coffee\",           \"me gusta el cafe\"),\n",
    "    (\"good morning\",            \"buenos dias\"),\n",
    "    (\"good night\",              \"buenas noches\"),\n",
    "    (\"how are you\",             \"como estas\"),\n",
    "    (\"thank you\",               \"gracias\"),\n",
    "    (\"you are welcome\",         \"de nada\"),\n",
    "    (\"i love nlp\",              \"me encanta el nlp\"),\n",
    "    (\"the dog is tired\",        \"el perro esta cansado\"),\n",
    "    (\"the cat is on the sofa\",  \"el gato esta en el sofa\"),\n",
    "    (\"today is sunny\",          \"hoy hace sol\"),\n",
    "    (\"i am learning pytorch\",   \"estoy aprendiendo pytorch\"),\n",
    "    (\"where is the bank\",       \"donde esta el banco\"),\n",
    "    (\"the park is green\",       \"el parque es verde\"),\n",
    "    (\"i go to the beach\",       \"voy a la playa\"),\n",
    "    (\"see you tomorrow\",        \"hasta manana\"),\n",
    "]\n",
    "\n",
    "vocab = build_vocab_from_parallel(pairs)\n",
    "print(\"Vocab size:\", len(vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4sy-9a0IlVV"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) Dataset seq2seq con teacher forcing\n",
    "#    src_idx  = [en_tokens] + [EOS]\n",
    "#    tgt_in   = [BOS] + [es_tokens]\n",
    "#    tgt_out  = [es_tokens] + [EOS]\n",
    "# -------------------------\n",
    "class EnEsDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[str, str]], vocab: Vocab):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        en, es = self.pairs[i]\n",
    "        src = encode_text(self.vocab, en) + [self.vocab.eos_id]\n",
    "        tgt_tokens = encode_text(self.vocab, es)\n",
    "        tgt_in  = [self.vocab.bos_id] + tgt_tokens\n",
    "        tgt_out = tgt_tokens + [self.vocab.eos_id]\n",
    "        return src, tgt_in, tgt_out\n",
    "\n",
    "def collate_seq2seq(batch, pad_id: int):\n",
    "    srcs, tgt_ins, tgt_outs = zip(*batch)\n",
    "    src = pad_2d(list(srcs), pad_id)\n",
    "    tgt_in = pad_2d(list(tgt_ins), pad_id)\n",
    "    tgt_out = pad_2d(list(tgt_outs), pad_id)\n",
    "    return src, tgt_in, tgt_out\n",
    "\n",
    "dl = DataLoader(\n",
    "    EnEsDataset(pairs, vocab),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_seq2seq(b, vocab.pad_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSL3vrfqIna4",
    "outputId": "dfab9d8b-8177-4ffb-a4b4-87c82b4bd5a6"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Entrenamiento con tu MiniTransformer\n",
    "#    (IMPORTANTE: ignorar PAD en la loss)\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MiniTransformer(\n",
    "    vocab_size=len(vocab.itos),\n",
    "    max_len=32,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    ffn_hidden=128,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "def train_steps(model, dl, steps=800, log_every=100):\n",
    "    model.train()\n",
    "    it = iter(dl)\n",
    "    losses = []\n",
    "    for step in range(1, steps + 1):\n",
    "        try:\n",
    "            src, tgt_in, tgt_out = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(dl)\n",
    "            src, tgt_in, tgt_out = next(it)\n",
    "\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits, _ = model(src, tgt_in, tgt_targets=None)  # (B,T,vocab)\n",
    "\n",
    "        # Loss token-a-token ignorando PAD\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            tgt_out.reshape(-1),\n",
    "            ignore_index=vocab.pad_id\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if step % log_every == 0:\n",
    "            print(f\"step {step:04d} | loss {loss.item():.4f}\")\n",
    "    return losses\n",
    "\n",
    "losses = train_steps(model, dl, steps=800, log_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Id1fcFiIup5"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5) Greedy decode EN->ES (autoregresivo)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def greedy_translate(model, src_idx: torch.Tensor, max_new_tokens: int = 20) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    src_idx = src_idx.to(device)\n",
    "\n",
    "    # Encoder\n",
    "    src = model.drop(model.pos_enc(model.tok_emb(src_idx)))\n",
    "    enc_out = model.encoder(src)\n",
    "\n",
    "    # Decoder empieza con BOS\n",
    "    B = src_idx.size(0)\n",
    "    ys = torch.full((B, 1), vocab.bos_id, dtype=torch.long, device=device)\n",
    "\n",
    "    finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        tgt = model.drop(model.pos_enc(model.tok_emb(ys)))\n",
    "        dec_out = model.decoder(tgt, enc_out)\n",
    "        logits_last = model.lm_head(dec_out)[:, -1, :]  # (B,vocab)\n",
    "        next_tok = logits_last.argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "\n",
    "        # si ya terminó una secuencia, la mantenemos en EOS (evita basura)\n",
    "        next_tok = torch.where(\n",
    "            finished.view(-1, 1),\n",
    "            torch.full_like(next_tok, vocab.eos_id),\n",
    "            next_tok\n",
    "        )\n",
    "\n",
    "        ys = torch.cat([ys, next_tok], dim=1)\n",
    "\n",
    "        finished |= (next_tok.squeeze(1) == vocab.eos_id)\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feDHcYwEIv9z",
    "outputId": "d19ca1f8-0b4e-42cf-cbec-8eea3e9889da"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 6) Prueba rápida\n",
    "# -------------------------\n",
    "def encode_src_sentence(vocab: Vocab, sent: str) -> torch.Tensor:\n",
    "    ids = encode_text(vocab, sent) + [vocab.eos_id]\n",
    "    return torch.tensor([ids], dtype=torch.long)\n",
    "\n",
    "tests = [\n",
    "    \"i like pizza\",\n",
    "    \"good morning\",\n",
    "    \"the dog is tired\",\n",
    "    \"i am learning pytorch\",\n",
    "    \"see you tomorrow\",\n",
    "]\n",
    "\n",
    "for s in tests:\n",
    "    src = encode_src_sentence(vocab, s)\n",
    "    pred = greedy_translate(model, src, max_new_tokens=20)[0].tolist()\n",
    "\n",
    "    # quitamos BOS y cortamos en EOS para imprimir bonito\n",
    "    if vocab.eos_id in pred:\n",
    "        pred_cut = pred[1: pred.index(vocab.eos_id)]\n",
    "    else:\n",
    "        pred_cut = pred[1:]\n",
    "\n",
    "    print(\"\\nEN:\", s)\n",
    "    print(\"ES(pred):\", decode_ids(vocab, pred_cut))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "gpt_chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
